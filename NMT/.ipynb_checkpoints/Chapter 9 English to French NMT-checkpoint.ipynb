{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Basic Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, WMT14\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "import itertools\n",
    "import allennlp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.attention import LinearAttention, CosineAttention, BilinearAttention, DotProductAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 100\n",
    "CLIP = 10\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "SAVE_DIR = 'exp_small_data_100'\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load('fr')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tokenizer when creating the initial data splits filtering\n",
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes French text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_fr, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, spacy pretrained tokenizers are used to load the English and German datasets.\n",
    "These spacy models can be used in conjunction with torchtext, allowing the processed data to populate torch tensors and dataset iterators to be created for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data to smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrenchTatoeba(TranslationDataset):\n",
    "    \"\"\"English-to-French dataset from Tatoeba\"\"\"\n",
    "\n",
    "    urls = ['https://download.pytorch.org/tutorial/data.zip']\n",
    "    name = 'FrenchTatoeba'\n",
    "    dirname = ''\n",
    "    \n",
    "    @classmethod\n",
    "    def format_data(cls, download_dir, lang1, lang2, reverse=False):\n",
    "        random.seed(1) # Get same split every time\n",
    "        print(\"Reading lines...\")\n",
    "\n",
    "        # Read the file and split into lines\n",
    "        lines = open(os.path.join(download_dir,'data/%s-%s.txt' % (lang1, lang2)), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "        # Split every line into pairs and normalize\n",
    "        pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "        # Reverse pairs\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "             \n",
    "        with open(os.path.join(download_dir,'all_data.en'), 'w') as lang1_file, \\\n",
    "                open(os.path.join(download_dir,'all_data.fr'), 'w') as lang2_file:\n",
    "            for p in pairs:\n",
    "                lang1_file.write(p[0] + '\\n')\n",
    "                lang2_file.write(p[1] + '\\n')\n",
    "\n",
    "    @classmethod\n",
    "    def all_data(cls, exts, fields, root='.data',\n",
    "               train='all_data', validation=None, test=None, **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the Tatoeba dataset.\n",
    "        Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "        \"\"\"\n",
    "        \n",
    "        if 'path' not in kwargs:\n",
    "            expected_folder = os.path.join(root, cls.name)\n",
    "            path = expected_folder if os.path.exists(expected_folder) else None\n",
    "        else:\n",
    "            path = kwargs['path']\n",
    "            del kwargs['path']\n",
    "        \n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "        \n",
    "        cls.format_data(path, 'eng', 'fra')\n",
    "\n",
    "        train_data = None if train is None else cls(\n",
    "            os.path.join(path, train), exts, fields, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            os.path.join(path, validation), exts, fields, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            os.path.join(path, test), exts, fields, **kwargs)\n",
    "        \n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, exts, fields, root='.data',\n",
    "               train='train', validation='val', test='test', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the Multi30k dataset.\n",
    "        Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        if 'path' not in kwargs:\n",
    "            expected_folder = os.path.join(root, cls.name)\n",
    "            path = expected_folder if os.path.exists(expected_folder) else None\n",
    "        else:\n",
    "            path = kwargs['path']\n",
    "            del kwargs['path']\n",
    "            \n",
    "        return super(FrenchTatoeba, cls).splits(\n",
    "            exts, fields, path, root, train, validation, test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes French text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    text = normalizeString(text)\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    text = normalizeString(text)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0]) < MAX_LENGTH and \\\n",
    "        len(p[1]) < MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = FrenchTatoeba.all_data(exts=('.en', '.fr'), \n",
    "                  fields=(SRC, TRG), \n",
    "                  filter_pred=lambda ex: filterPair([ex.src, ex.trg]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in all_data[:10]:\n",
    "    print(e.src, e.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = os.path.join('.data', FrenchTatoeba.name)\n",
    "n_examples = len(all_data)\n",
    "idx_array = list(range(n_examples))\n",
    "random.shuffle(idx_array)\n",
    "train_indexs = idx_array[:int(0.8*n_examples)] # 80% training data\n",
    "val_indexs = idx_array[int(0.8*n_examples):int(0.9*n_examples)]\n",
    "test_indexs = idx_array[int(0.9*n_examples):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save train, test, val files for furture experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(download_dir,'train.en'), 'w') as lang1_file, \\\n",
    "        open(os.path.join(download_dir,'train.fr'), 'w') as lang2_file:\n",
    "    for i in train_indexs:\n",
    "        lang1_file.write(' '.join(all_data[i].src) + '\\n')\n",
    "        lang2_file.write(' '.join(all_data[i].trg) + '\\n')\n",
    "\n",
    "with open(os.path.join(download_dir,'val.en'), 'w') as lang1_file, \\\n",
    "        open(os.path.join(download_dir,'val.fr'), 'w') as lang2_file:\n",
    "    for i in val_indexs:\n",
    "        lang1_file.write(' '.join(all_data[i].src) + '\\n')\n",
    "        lang2_file.write(' '.join(all_data[i].trg) + '\\n')\n",
    "\n",
    "with open(os.path.join(download_dir,'test.en'), 'w') as lang1_file, \\\n",
    "        open(os.path.join(download_dir,'test.fr'), 'w') as lang2_file:\n",
    "    for i in test_indexs:\n",
    "        lang1_file.write(' '.join(all_data[i].src) + '\\n')\n",
    "        lang2_file.write(' '.join(all_data[i].trg) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load individual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = FrenchTatoeba.splits(path='./.data/FrenchTatoeba/', exts=('.en', '.fr'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=5)\n",
    "TRG.build_vocab(train_data, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Target distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set size: ', len(train_data))\n",
    "print('Validation set size: ', len(valid_data))\n",
    "print('Testing set size: ', len(test_data))\n",
    "\n",
    "print('Size of English vocabulary: ', len(SRC.vocab))\n",
    "print('Size of French vocabulary: ', len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dual_hist(data, data_src_title, data_tgt_title, title):\n",
    "    data_1 = np.asarray([len(x.src) for x in data])\n",
    "    data_2 = np.asarray([len(x.trg) for x in data])\n",
    "    max_len = max(max(data_1),max(data_2))\n",
    "    bins = range(1, max_len + 1, 1)\n",
    "    plt.hist([data_1, data_2], bins, label=[data_src_title, data_tgt_title], align='left')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, max_len))\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Number of examples\")\n",
    "    plt.xlabel(\"Example label\")\n",
    "    plt.figure(figsize=(180, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "show_dual_hist(train_data,\n",
    "               'English', \n",
    "               'French', \n",
    "               \"Training Sentence Lengths\")\n",
    "show_dual_hist(valid_data,\n",
    "               'English', \n",
    "               'French', \n",
    "               \"Validation Sentence Lengths\")\n",
    "show_dual_hist(test_data,\n",
    "               'English', \n",
    "               'French', \n",
    "               \"Testing Sentence Lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_bar_plot(data, data_title, top_n=100):\n",
    "    objects = sorted(data.freqs, key=data.freqs.get, reverse=True)[0:top_n]\n",
    "    counts = [data.freqs[o] for o in objects]\n",
    "    y_pos = np.arange(len(objects))\n",
    "\n",
    "    plt.barh(y_pos, counts, align='center', alpha=0.5)\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Count')\n",
    "    plt.title(data_title + \" Top \" + str(top_n) + \" word counts\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [7, 18]\n",
    "word_frequency_bar_plot(SRC.vocab, 'English', top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [7, 18]\n",
    "word_frequency_bar_plot(TRG.vocab, 'French', top_n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Utilities for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows us to optimize the encoder and decoder separately\n",
    "class MultipleOptimizer(object):\n",
    "    def __init__(self, *op):\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for op in self.optimizers:\n",
    "            op.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(total=len(iterator)) as t:\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            if clip: \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            t.set_postfix(loss='{:05.3f}'.format(epoch_loss / len(iterator)))\n",
    "            t.update()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "  \n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_EPOCHS+1)\n",
    "\n",
    "    plt.plot(e, metric, color='navy', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, color='red', label='Validation ' + metric_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict, title, chart_filter=''):\n",
    "    data = pd.DataFrame.from_dict(metrics_dict)\n",
    "    data = data.T\n",
    "    data = data[list(filter(re.compile('.*'+ chart_filter +'.*').match, \n",
    "                            list(data.columns.values)))]\n",
    "    data.plot(figsize=(10,6), title=title).legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, text):\n",
    "    model.eval()\n",
    "    batch_size = BATCH_SIZE\n",
    "    with torch.no_grad():\n",
    "        src = SRC.process([SRC.preprocess(text)]).to(device)\n",
    "        max_len = 2* len(src)\n",
    "        encoder_outputs, hidden  = model.encoder(src)\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = TRG.process([TRG.preprocess(' ')])[0,:].to(device)\n",
    "\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, len(TRG.vocab)).to(device)\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_len, len(src))\n",
    "\n",
    "        for t in range(0, max_len):\n",
    "            output, hidden, attn = model.decoder(output, hidden, encoder_outputs)\n",
    "\n",
    "            decoder_attentions[t] = attn.data\n",
    "            outputs[t] = output\n",
    "            teacher_force = 0 #random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "            \n",
    "            if top1 == TRG.vocab.stoi['<eos>']:\n",
    "                decoded_words.append('<eos>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(TRG.vocab.itos[top1])\n",
    "            \n",
    "        output_sentence = ' '.join(decoded_words)\n",
    "        return output_sentence, decoder_attentions[:t + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_sentence, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy())\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels(['', '<sos>'] + input_sentence.split(' ') +\n",
    "                       ['<eos>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_sentence.split(' '))\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(model, input_sentence):\n",
    "    output_words, attentions = predict_text(model, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output_words)\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, num_layers=2, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        \n",
    "        #outputs = [sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2))) \n",
    "        \n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "                \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        attns = self.attention(hidden, encoder_outputs)       \n",
    "        #attn = [batch size, src len]\n",
    "        \n",
    "        a = attns.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0), attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, _ = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "\n",
    "enc = BahdanauEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = BahdanauDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "learning_rate = 0.001\n",
    "decoder_learning_ratio = 1.0\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.SGD(enc.parameters(), lr=learning_rate, momentum=0.99)\n",
    "decoder_optimizer = optim.SGD(dec.parameters(), lr=learning_rate * decoder_learning_ratio, momentum=0.9)\n",
    "enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', patience=5)\n",
    "dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', patience=5)\n",
    "\n",
    "optimizer = MultipleOptimizer(encoder_optimizer, \n",
    "                        decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# N_EPOCHS = 5\n",
    "# CLIP = 10\n",
    "# SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'bahdanau_model.pt')\n",
    "bahdanau_training_metrics = {}\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')\n",
    "    bahdanau_training_metrics[epoch] = {'train_loss':train_loss, 'train_ppl':math.exp(train_loss), 'val_loss':valid_loss, 'val_ppl':math.exp(valid_loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(bahdanau_training_metrics, \"Bahdanau Attention\", chart_filter='loss')\n",
    "plot_metrics(bahdanau_training_metrics, \"Bahdanau Attention\", chart_filter='ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'bahdanau_model.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "evaluateAndShowAttention(model, 'you should try to see it .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i m shorter than you .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'it was difficult for me .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i don t follow .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'the region is relatively rich in mineral resources .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i took many photos and some of them were printed in black and white .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'bahdanau_model.json'), 'w') as fp:\n",
    "    json.dump(bahdanau_training_metrics, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Global Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, \n",
    "                 dropout, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, num_layers=num_layers, bidirectional=bidirectional)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "            \n",
    "        if not self.bidirectional and self.num_layers > 1: \n",
    "            hidden = hidden[-1,:,:]                   \n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, \n",
    "                 attention, bidirectional_input=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        self.bidirectional_input = bidirectional_input\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        if bidirectional_input:\n",
    "            self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "            self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        else:\n",
    "            self.rnn = nn.GRU((enc_hid_dim) + emb_dim, dec_hid_dim)\n",
    "            self.out = nn.Linear((enc_hid_dim) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        hidden = hidden.squeeze(0) if len(hidden.size()) > 2 else hidden # batch_size=1 issue\n",
    "       \n",
    "        # Repeat hidden state for attention on bidirectional outputs\n",
    "        if hidden.size(-1) != encoder_outputs.size(-1):\n",
    "#             print(\"Hidden size: \", hidden.size())\n",
    "#             print(\"Encoder outputs size: \", encoder_outputs.size())\n",
    "            attn = self.attention(hidden.repeat(1, 2), encoder_outputs.permute(1, 0, 2))\n",
    "        else:\n",
    "            attn = self.attention(hidden, encoder_outputs.permute(1, 0, 2))\n",
    "\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = attn.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        hidden = hidden.squeeze(1)\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "#             print(\"Hidden \", hidden.size())\n",
    "            output, hidden, attn = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_bidirectional = True\n",
    "enc_num_layers = 4\n",
    "\n",
    "attn = DotProductAttention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, \n",
    "              num_layers=enc_num_layers, bidirectional=enc_bidirectional)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, \n",
    "              attn, bidirectional_input=enc_bidirectional)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.SGD(enc.parameters(), lr=learning_rate, momentum=0.9)\n",
    "decoder_optimizer = optim.SGD(dec.parameters(), lr=learning_rate * decoder_learning_ratio, momentum=0.9)\n",
    "enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', patience=5)\n",
    "dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', patience=5)\n",
    "\n",
    "optimizer = MultipleOptimizer(encoder_optimizer, \n",
    "                        decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'dot_model.pt')\n",
    "dot_training_metrics = {}\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')\n",
    "    dot_training_metrics[epoch] = {'train_loss':train_loss, 'train_ppl':math.exp(train_loss), 'val_loss':valid_loss, 'val_ppl':math.exp(valid_loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(dot_training_metrics, \"Dot Product Attention\", chart_filter='loss')\n",
    "plot_metrics(dot_training_metrics, \"Dot Product Attention\", chart_filter='ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'dot_model.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'dot_model.json'), 'w') as fp:\n",
    "    json.dump(dot_training_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "evaluateAndShowAttention(model, 'you should try to see it .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i m shorter than you .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'it was difficult for me .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i don t follow .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'the region is relatively rich in mineral resources .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i took many photos and some of them were printed in black and white .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Cosine Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_bidirectional = True\n",
    "enc_num_layers = 4\n",
    "\n",
    "attn = CosineAttention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, \n",
    "              num_layers=enc_num_layers, bidirectional=enc_bidirectional)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, \n",
    "              attn, bidirectional_input=enc_bidirectional)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.SGD(enc.parameters(), lr=learning_rate, momentum=0.9)\n",
    "decoder_optimizer = optim.SGD(dec.parameters(), lr=learning_rate * decoder_learning_ratio, momentum=0.9)\n",
    "enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', patience=5)\n",
    "dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', patience=5)\n",
    "\n",
    "optimizer = MultipleOptimizer(encoder_optimizer, \n",
    "                        decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'cosine_model.pt')\n",
    "cosine_training_metrics = {}\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')\n",
    "    cosine_training_metrics[epoch] = {'train_loss':train_loss, 'train_ppl':math.exp(train_loss), 'val_loss':valid_loss, 'val_ppl':math.exp(valid_loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(cosine_training_metrics, \"Cosine Product Attention\", chart_filter='loss')\n",
    "plot_metrics(cosine_training_metrics, \"Cosine Product Attention\", chart_filter='ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'cosine_model.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'cosine_model.json'), 'w') as fp:\n",
    "    json.dump(cosine_training_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(model, 'you should try to see it .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i m shorter than you .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'it was difficult for me .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i don t follow .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'the region is relatively rich in mineral resources .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i took many photos and some of them were printed in black and white .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Bilinear Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_bidirectional = True\n",
    "enc_num_layers = 4\n",
    "\n",
    "# attn = BilinearAttention(ENC_HID_DIM, DEC_HID_DIM) # Uni-directional attention\n",
    "attn = BilinearAttention((ENC_HID_DIM * 2), (ENC_HID_DIM * 2)) # Bi-directional attention\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, \n",
    "              num_layers=enc_num_layers, bidirectional=enc_bidirectional)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, \n",
    "              attn, bidirectional_input=enc_bidirectional)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.SGD(enc.parameters(), lr=learning_rate, momentum=0.9)\n",
    "decoder_optimizer = optim.SGD(dec.parameters(), lr=learning_rate * decoder_learning_ratio, momentum=0.9)\n",
    "enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', patience=5)\n",
    "dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', patience=5)\n",
    "\n",
    "optimizer = MultipleOptimizer(encoder_optimizer, \n",
    "                        decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'bilinear_model_bidirectional.pt')\n",
    "bilinear_training_metrics = {}\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')\n",
    "    bilinear_training_metrics[epoch] = {'train_loss':train_loss, 'train_ppl':math.exp(train_loss), 'val_loss':valid_loss, 'val_ppl':math.exp(valid_loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(bilinear_training_metrics, \"Bilinear Product Attention\", chart_filter='loss')\n",
    "plot_metrics(bilinear_training_metrics, \"Bilinear Product Attention\", chart_filter='ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'bilinear_model.json'), 'w') as fp:\n",
    "    json.dump(bilinear_training_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'bilinear_model_bidirectional.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(model, 'you should try to see it .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i m shorter than you .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'it was difficult for me .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i don t follow .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'the region is relatively rich in mineral resources .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i took many photos and some of them were printed in black and white .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 Linear Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_bidirectional = True\n",
    "enc_num_layers = 4\n",
    "\n",
    "# attn = BilinearAttention(ENC_HID_DIM, DEC_HID_DIM) # Uni-directional attention\n",
    "attn = LinearAttention((ENC_HID_DIM * 2), (ENC_HID_DIM * 2)) # Bi-directional attention\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, \n",
    "              num_layers=enc_num_layers, bidirectional=enc_bidirectional)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, \n",
    "              attn, bidirectional_input=enc_bidirectional)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.SGD(enc.parameters(), lr=learning_rate, momentum=0.9)\n",
    "decoder_optimizer = optim.SGD(dec.parameters(), lr=learning_rate * decoder_learning_ratio, momentum=0.9)\n",
    "enc_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, 'min', patience=5)\n",
    "dec_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, 'min', patience=5)\n",
    "\n",
    "optimizer = MultipleOptimizer(encoder_optimizer, \n",
    "                        decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'linear_model.pt')\n",
    "linear_training_metrics = {}\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')\n",
    "    linear_training_metrics[epoch] = {'train_loss':train_loss, 'train_ppl':math.exp(train_loss), 'val_loss':valid_loss, 'val_ppl':math.exp(valid_loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(linear_training_metrics, \"Linear Product Attention\", chart_filter='loss')\n",
    "plot_metrics(linear_training_metrics, \"Linear Product Attention\", chart_filter='ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'linear_model.pt')))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'linear_model.json'), 'w') as fp:\n",
    "    json.dump(linear_training_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(model, 'you should try to see it .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i m shorter than you .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'it was difficult for me .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i don t follow .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'the region is relatively rich in mineral resources .')\n",
    "\n",
    "evaluateAndShowAttention(model, 'i took many photos and some of them were printed in black and white .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparison of Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergedicts(dict1, dict2):\n",
    "    for k in set(dict1.keys()).union(dict2.keys()):\n",
    "        if k in dict1 and k in dict2:\n",
    "            if isinstance(dict1[k], dict) and isinstance(dict2[k], dict):\n",
    "                yield (k, dict(mergedicts(dict1[k], dict2[k])))\n",
    "            else:\n",
    "                # If one of the values is not a dict, you can't continue merging it.\n",
    "                # Value from second dict overrides one in first and we move on.\n",
    "                yield (k, dict2[k])\n",
    "                # Alternatively, replace this with exception raiser to alert you of value conflicts\n",
    "        elif k in dict1:\n",
    "            yield (k, dict1[k])\n",
    "        else:\n",
    "            yield (k, dict2[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "with open('./exp_small_data_100/bahdanau_model.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e, l in data.items():\n",
    "        new_data = {}\n",
    "        for k,v in l.items():\n",
    "            new_data[\"bahdanau_\" + k] = v\n",
    "        data[e] = new_data\n",
    "    all_results = dict(mergedicts(all_results, data))\n",
    "\n",
    "with open('./exp_small_data_100/dot_model.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e, l in data.items():\n",
    "        new_data = {}\n",
    "        for k,v in l.items():\n",
    "            new_data[\"dot_\" + k] = v\n",
    "        data[e] = new_data\n",
    "    all_results = dict(mergedicts(all_results, data))\n",
    "\n",
    "with open('./exp_small_data_100/cosine_model.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e, l in data.items():\n",
    "        new_data = {}\n",
    "        for k,v in l.items():\n",
    "            new_data[\"cosine_\" + k] = v\n",
    "        data[e] = new_data\n",
    "    all_results = dict(mergedicts(all_results, data))\n",
    "\n",
    "with open('./exp_small_data_100/bilinear_model.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e, l in data.items():\n",
    "        new_data = {}\n",
    "        for k,v in l.items():\n",
    "            new_data[\"bilinear_\" + k] = v\n",
    "        data[e] = new_data\n",
    "    all_results = dict(mergedicts(all_results, data))\n",
    "\n",
    "with open('./exp_small_data_100/linear_model.json') as f:\n",
    "    data = json.load(f)\n",
    "    for e, l in data.items():\n",
    "        new_data = {}\n",
    "        for k,v in l.items():\n",
    "            new_data[\"linear_\" + k] = v\n",
    "        data[e] = new_data\n",
    "    all_results = dict(mergedicts(all_results, data))\n",
    "\n",
    "# Ensure keys are sorted as integers instead of strings\n",
    "all_results = {int(k):v for k,v in all_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(all_results, \"Attention Comparison: Training PPL\", \n",
    "             'train_ppl')\n",
    "plot_metrics(all_results, \"Attention Comparison: Validation PPL\", \n",
    "             'val_ppl')\n",
    "\n",
    "plot_metrics(all_results, \"Attention Comparison: Training Loss\", \n",
    "             'train_loss')\n",
    "plot_metrics(all_results, \"Attention Comparison: Validation Loss\", \n",
    "             'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.metrics import get_moses_multi_bleu\n",
    "\n",
    "hypotheses = [\"The brown fox jumps over the dog 笑\"]\n",
    "references = [\"The quick brown fox jumps over the lazy dog 笑\"]\n",
    "\n",
    "# Compute BLEU score with the official BLEU perl script\n",
    "get_moses_multi_bleu(hypotheses, references, lowercase=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
