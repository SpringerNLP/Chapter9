{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-v8wLC-vrwZ"
   },
   "source": [
    "# Question and Answering\n",
    "\n",
    "To help the reader familiarize themselves with the attention and memory networks, we will apply the concepts of this chapter to the question answering task with the bAbI dataset. The bAbI is a collection of 20 simple QA tasks with limited vocabulary (https://research.fb.com/downloads/babi). For each task, there is a set of 1000 training and 1000 stories, test questions and answers as well as an extended training set with 10,000 samples. Despite its simplicity, bAbI effectively captures the complexities of memory and long-range dependencies in question answering. For this case study, we will focus on tasks 1-3, consisting of questions where a single, two, or three supporting facts from the stories provide information to support the answer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGLTkm-d2OOg"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Our first step is to download the bAbI dataset and to extract the training and test sets for our analysis. We will focus on the extended dataset with 10,000 training samples and 1000 test samples. Analysis of the datasets shows the increasing complexity and long-range memory that is required when progressing from task QA1 to QA3. Lets examine the distribution of story lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRKC-i0k2JBn"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                 substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return (pad_sequences(xs, maxlen=story_maxlen),\n",
    "            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "myWHLDUG2pXn",
    "outputId": "81d089ec-c2b1-422e-b147-fc1f03423343"
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "path = get_file('babi-tasks-v1-2.tar.gz',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "\n",
    "challenge1 = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
    "challenge2 = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
    "challenge3 = 'tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt'\n",
    "\n",
    "with tarfile.open(path) as tar:\n",
    "    train1 = get_stories(tar.extractfile(challenge1.format('train')))\n",
    "    test1 = get_stories(tar.extractfile(challenge1.format('test')))\n",
    "    train2 = get_stories(tar.extractfile(challenge2.format('train')))\n",
    "    test2 = get_stories(tar.extractfile(challenge2.format('test')))\n",
    "    train3 = get_stories(tar.extractfile(challenge3.format('train')))\n",
    "    test3 = get_stories(tar.extractfile(challenge3.format('test')))\n",
    "\n",
    "\n",
    "qa1_story_len=[len(s) for s, q, a in train1+test1]\n",
    "qa1_query_len=[len(q) for s, q, a in train1+test1]\n",
    "qa2_story_len=[len(s) for s, q, a in train2+test2]\n",
    "qa2_query_len=[len(q) for s, q, a in train2+test2]\n",
    "qa3_story_len=[len(s) for s, q, a in train3+test3]\n",
    "qa3_query_len=[len(q) for s, q, a in train3+test3]\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train1 + test1:\n",
    "    vocab |= set(story + q + [answer])\n",
    "qa1_vocab = sorted(vocab)\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train2 + test2:\n",
    "    vocab |= set(story + q + [answer])\n",
    "qa2_vocab = sorted(vocab)\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train3 + test3:\n",
    "    vocab |= set(story + q + [answer])\n",
    "qa3_vocab = sorted(vocab)\n",
    "\n",
    "print()\n",
    "print('QA1 Story: \"{}\"'.format(\" \".join(train1[0][0])))\n",
    "print('QA1 Query: \"{}\"'.format(\" \".join(train1[0][1])))\n",
    "print('QA1 Answer: \"{}\"'.format(train1[0][2]))\n",
    "print()\n",
    "print('QA2 Story: \"{}\"'.format(\" \".join(train2[0][0])))\n",
    "print('QA2 Query: \"{}\"'.format(\" \".join(train2[0][1])))\n",
    "print('QA2 Answer: \"{}\"'.format(train2[0][2]))\n",
    "print()\n",
    "print('QA3 Story: \"{}\"'.format(\" \".join(train3[0][0])))\n",
    "print('QA3 Query: \"{}\"'.format(\" \".join(train3[0][1])))\n",
    "print('QA3 Answer: \"{}\"'.format(train3[0][2]))\n",
    "print()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['task'] = [\"qa1\",\"qa2\",\"qa3\"]\n",
    "df['train_stories'] = [len(train1),len(train2),len(train3)]\n",
    "df['test_stories'] = [len(test1),len(test2),len(test3)]\n",
    "df['min(story_size)'] = [min(qa1_story_len),min(qa2_story_len),min(qa3_story_len)]\n",
    "df['max(story_size)'] = [max(qa1_story_len),max(qa2_story_len),max(qa3_story_len)]\n",
    "df['query_size'] = [qa1_query_len[0],qa2_query_len[0],qa3_query_len[0]]\n",
    "df['vocab_size'] = [len(qa1_vocab),len(qa2_vocab),len(qa3_vocab)]\n",
    "display(HTML(df.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbT7Cd854QSS"
   },
   "source": [
    "The average length of the stories increases substantially from tasks QA1 to QA3, which makes it significantly more difficult. Remember that for task QA3, there are only three support facts and most of the story is considered “noise.” We will see how well different architectures are able to learn to identify the relevant facts from this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "wniqEHsC4U5T",
    "outputId": "08272661-e348-4dff-d016-c41a07a32eed"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_bins = 40\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "ax1.hist(qa1_story_len, num_bins, facecolor='blue', alpha=0.5)\n",
    "ax1.set_title('Story Length Distribution')\n",
    "ax2.hist(qa2_story_len, num_bins, facecolor='red', alpha=0.5)\n",
    "ax3.hist(qa3_story_len, num_bins, facecolor='green', alpha=0.5)\n",
    "f.subplots_adjust(hspace=0)\n",
    "\n",
    "num_bins = 8\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "ax1.hist(qa1_query_len, num_bins, facecolor='blue', alpha=0.5)\n",
    "ax1.set_title('Query Length Distribution')\n",
    "ax2.hist(qa2_query_len, num_bins, facecolor='red', alpha=0.5)\n",
    "ax3.hist(qa3_query_len, num_bins, facecolor='green', alpha=0.5)\n",
    "f.subplots_adjust(hspace=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilRxFbYKvrwa"
   },
   "source": [
    "## LSTM Baseline\n",
    "\n",
    "We take the Keras example LSTM architecture to serve as our baseline (https://github.com/keras-team/keras). This architecture consists of the following:\n",
    "\n",
    "1. The tokens of each story and question are mapped to embeddings (that are not shared between them)\n",
    "2. The stories and questions are encoded using separate LSTMs\n",
    "3. The encoded vectors for the story and question are concatenated\n",
    "4. These concatenated vectors are used as an input to a DNN whose output is a softmax over the vocabulary\n",
    "5. the entire network is trained to minimize the error between the softmax output and the answer.\n",
    "\n",
    "We train this model using the extended bAbI training sets with 50-dim embeddings, 100-dim encodings, batch size of 32, and the adam optimizer for 100 epochs. As seen in the results, the longer the stories, the worse the performance of the LSTM model due to the increased “noise” in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byeXcP_D4lx8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent, Dropout\n",
    "\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "TASK = 1     # Choose 1,2, or 3 for task QA1, QA2, or QA3\n",
    "\n",
    "print('Embed = {}'.format(EMBED_HIDDEN_SIZE))\n",
    "print('Sent = {}'.format(SENT_HIDDEN_SIZE))\n",
    "print('Query = {}'.format(QUERY_HIDDEN_SIZE))\n",
    "\n",
    "if TASK == 1:\n",
    "  vocab = qa1_vocab\n",
    "  train = train1\n",
    "  test = test1\n",
    "elif TASK == 2:\n",
    "  vocab = qa2_vocab\n",
    "  train = train2\n",
    "  test = test2\n",
    "else:\n",
    "  vocab = qa3_vocab\n",
    "  train = train3\n",
    "  test = test3\n",
    "  \n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "\n",
    "sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n",
    "encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = RNN(SENT_HIDDEN_SIZE,return_sequences=False)(encoded_sentence)\n",
    "\n",
    "question = layers.Input(shape=(query_maxlen,), dtype='int32')\n",
    "encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n",
    "encoded_question = RNN(QUERY_HIDDEN_SIZE,return_sequences=False)(encoded_question)\n",
    "\n",
    "merged = layers.concatenate([encoded_sentence,encoded_question])\n",
    "preds = layers.Dense(vocab_size, activation='softmax')(merged)\n",
    "\n",
    "model = Model([sentence, question], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x, xq], y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "\n",
    "print('Evaluation')\n",
    "\n",
    "loss, acc = model.evaluate([tx, txq], ty,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBJVMM0q60Sz"
   },
   "source": [
    "Try running the above code for Tasks 1-3 and experiment with different hyperparameter settings to improve test set accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zbv8q2Qvrwf"
   },
   "source": [
    "## End-to-end Memory Network\n",
    "\n",
    "Memory networks offer the opportunity store long-term information and thereby improve performance, especially on longer sequences such as task QA3. Memory networks are able to store supporting facts as memory vectors which are queried and used for prediction. In the original form by Weston, the memory vectors are learned via direct supervision with hard attention and supervision is required at each layer of the network. This requires significant effort. To overcome this need, end- to-end memory networks as proposed by Sukhbaatar use soft attention in place of supervision that can be learned during training via backpropagation. This end-to-end architecture takes the following steps:\n",
    "\n",
    "1. Each story sentence and query are mapped to separate embedding representations\n",
    "2. The query embedding is compared with the embedding of each sentence in the memory, and a softmax function is used to generate a probability distribution analogous to a soft attention mechanism\n",
    "3. These probabilities are used to select the most relevant sentence in memory using a separate set of sentence embeddings\n",
    "4. The resulting vector is concatenated with the query embedding and used as input to a LSTM layer followed by a Dense Layer with a softmax output\n",
    "5. The entire network is trained to minimize the error between the softmax output and answer\n",
    "\n",
    "Note that this is termed a 1-hop or single layered MemN2N, since we query the memory only once. As described earlier, memory layers can be stacked to improve performance, especially where multiple facts are relevant and necessary to predict the answer.\n",
    "\n",
    "We train this single-layered Keras model (https://github.com/keras-team/keras) using the extended bAbI training sets with 50- dim embeddings, batch size of 32, and the adam optimizer for 100 epochs. In comparsion to the baseline LSTM, the MemN2N model did significantly better for all three tasks, and especially for QA1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtxfT1RQvrwf"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "TASK = 1\n",
    "\n",
    "if TASK == 1:\n",
    "  vocab = qa1_vocab\n",
    "  train_stories = train1\n",
    "  test_stories = test1\n",
    "elif TASK == 2:\n",
    "  vocab = qa2_vocab\n",
    "  train_stories = train2\n",
    "  test_stories = test2\n",
    "else:\n",
    "  vocab = qa3_vocab\n",
    "  train_stories = train3\n",
    "  test_stories = test3\n",
    "\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n",
    "  \n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
    "\n",
    "# Build model\n",
    "\n",
    "input_sequence = Input((story_maxlen,))\n",
    "input_encoded_m = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=EMBED_HIDDEN_SIZE)(input_sequence)\n",
    "input_encoded_m = Dropout(0.3)(input_encoded_m)\n",
    "\n",
    "input_encoded_c = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=query_maxlen)(input_sequence)\n",
    "input_encoded_c = Dropout(0.3)(input_encoded_c)\n",
    "\n",
    "question = Input((query_maxlen,))\n",
    "question_encoded = Embedding(input_dim=vocab_size,\n",
    "                             output_dim=EMBED_HIDDEN_SIZE,\n",
    "                             input_length=query_maxlen)(question)\n",
    "question_encoded = Dropout(0.3)(question_encoded)\n",
    "\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "response = add([match, input_encoded_c])\n",
    "response = Permute((2, 1))(response)\n",
    "\n",
    "answer = concatenate([response, question_encoded])\n",
    "answer = LSTM(BATCH_SIZE)(answer)\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "loss, acc = model.evaluate([inputs_test, queries_test], answers_test,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJ3Fp9_LC8J_"
   },
   "source": [
    "Try running the above code for Tasks 1-3 and experiment with different hyperparameter settings to improve test set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qai-U8xSvrwh"
   },
   "source": [
    "## Dynamic Memory Network\n",
    "\n",
    "As discussed earlier, dynamic memory networks take memory networks one step further and encode memories using a GRU layer. An episodic memory layer is the key to dynamic memory networks, with its attention mechanisms for feature gener- ation and scoring. Episodic memory is composed itself by two nested GRUs, where the inner GRU generates the episodes and the outer GRU generates the memory vector from the sequence of episodes. DMNs follow the following steps:\n",
    "\n",
    "1. The input story sentences and query are encoded using GRUs and passed to the episodic memory module\n",
    "2. Episodes are generated by attending over these encodings to form a memory such that sentence encodings with low attention scores are ignored\n",
    "3. Episodes along with previous memory states are used to update the episodic memory\n",
    "4. The query and memory states serve as inputs to the GRU within the answer module which is used to predict the output\n",
    "5. The entire network is trained to minimize the error between the GRU output and answer\n",
    "\n",
    "A tensorFlow implementation of the episodic memory module for a dynamic mem- ory network is provided below. Note that EpisodicMemoryModule depends on a soft attention GRU implementation.\n",
    "\n",
    "We train a DMN model (https://github.com/vchudinov/dynamic_memory_networks_with_keras) using the extended bAbI training sets with 50-dim GloVe embeddings, batch size of 50, 100 hidden units, 3 memory steps and the adam opti- mizer for just 20 epochs. In comparison with earlier architectures, we can see that dynamic memory networks perform better than MemN2N and LSTM networks for all three tasks, reaching perfect prediction on task QA1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdUTNWQ1vrwh"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.python.ops import array_ops\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Bidirectional, Dropout\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "import numpy as np\n",
    "import os, re\n",
    "from functools import reduce\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "HIDDEN_SIZE = 100\n",
    "MEM_STEPS = 3\n",
    "EPOCHS = 20\n",
    "\n",
    "TASK = 1\n",
    "\n",
    "\n",
    "def get_positional_encoding(max_seq, emb_dim):\n",
    "    encoding = np.ones((emb_dim, max_seq), dtype=np.float32)\n",
    "    ls = max_seq + 1\n",
    "    le = emb_dim + 1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            encoding[i - 1, j - 1] = (i - (le - 1) / 2) * (j - (ls - 1) / 2)\n",
    "    encoding = 1 + 4 * encoding / emb_dim / max_seq\n",
    "    return np.transpose(encoding)\n",
    "\n",
    "\n",
    "def load_embeddings_index(embeddings_path, emb_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(embeddings_path, 'r', encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = values[1:emb_dim]\n",
    "        coefs = np.asarray(coefs, dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    embeddings_index[\"<eos>\"] = np.random.rand(len(coefs))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    s = [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "    s = [x if x != \".\" else \"<eos>\" for x in s]\n",
    "    s = [x.lower() for x in s if x != \"?\"]\n",
    "    return s\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "\n",
    "    def flatten(data): return reduce(lambda x, y: x + y, data)\n",
    "    data = [\n",
    "        (flatten(story),\n",
    "         q,\n",
    "         answer) for story,\n",
    "        q,\n",
    "        answer in data if not max_length or len(\n",
    "            flatten(story)) < max_length]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    task_labels = sorted(list(set([x for _, _, x in data])))\n",
    "    positional_encoding = get_positional_encoding(story_maxlen, len(word_idx[\"<eos>\"]))\n",
    "\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = np.eye(len(task_labels))[task_labels.index(answer)]\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    xs = pad_sequences(xs, maxlen=story_maxlen)\n",
    "    xs = [x * positional_encoding for x in xs]\n",
    "    return np.array(xs), pad_sequences(xqs, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "\n",
    "def load_dataset( emb_location, babi_location, babi_test_location=None, emb_dim=50):\n",
    "    print(\"Loading Embeddings...\")\n",
    "    word_index = load_embeddings_index(emb_location, emb_dim)\n",
    "    print(\"Retrieving Stories...\")\n",
    "    stories = get_stories(open(babi_location, 'r'))\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in stories)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in stories)))\n",
    "\n",
    "\n",
    "    if babi_test_location is not None:\n",
    "        test_stories = get_stories(open(babi_test_location, 'r'))\n",
    "        test_story_maxlen = max(map(len, (x for x, _, _ in test_stories)))\n",
    "        test_query_maxlen = max(map(len, (x for _, x, _ in test_stories)))\n",
    "        story_maxlen = max(story_maxlen, test_story_maxlen)\n",
    "        query_maxlen = max(query_maxlen, test_query_maxlen)\n",
    "        vectorized_test = vectorize_stories(test_stories, word_index, story_maxlen, query_maxlen)\n",
    "\n",
    "    vectorized_stories = vectorize_stories(\n",
    "        stories, word_index, story_maxlen, query_maxlen)\n",
    "    if babi_test_location is not None:\n",
    "        return story_maxlen, vectorized_stories, vectorized_test\n",
    "\n",
    "    return story_maxlen, vectorized_stories, None\n",
    "  \n",
    "\n",
    "class SoftAttnGRU(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation='tanh',\n",
    "                 recurrent_activation='hard_sigmoid',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_constraint=None,\n",
    "                 recurrent_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 dropout=0.,\n",
    "                 recurrent_dropout=0.,\n",
    "                 implementation=1,\n",
    "                 return_sequences=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(SoftAttnGRU, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.return_sequences = return_sequences\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1., max(0., dropout))\n",
    "        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n",
    "        self.implementation = implementation\n",
    "        self.state_size = self.units\n",
    "        self._dropout_mask = None\n",
    "        self._recurrent_dropout_mask = None\n",
    "\n",
    "        self._input_map = {}\n",
    "\n",
    "        super(SoftAttnGRU, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        out = list(input_shape)\n",
    "        out[-1] = self.units\n",
    "        if self.return_sequences:\n",
    "            return out\n",
    "        else:\n",
    "            return (out[0], out[-1])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        input_dim = input_shape[-1] - 1\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units * 3),\n",
    "                                      name='kernel',\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 3),\n",
    "            name='recurrent_kernel',\n",
    "            initializer=self.recurrent_initializer,\n",
    "            constraint=self.recurrent_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units * 3,),\n",
    "                                        name='bias',\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.kernel_z = self.kernel[:, :self.units]\n",
    "        self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n",
    "        self.kernel_r = self.kernel[:, self.units: self.units * 2]\n",
    "        self.recurrent_kernel_r = self.recurrent_kernel[:,\n",
    "                                                        self.units:\n",
    "                                                        self.units * 2]\n",
    "        self.kernel_h = self.kernel[:, self.units * 2:]\n",
    "        self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias_z = self.bias[:self.units]\n",
    "            self.bias_r = self.bias[self.units: self.units * 2]\n",
    "            self.bias_h = self.bias[self.units * 2:]\n",
    "        else:\n",
    "            self.bias_z = None\n",
    "            self.bias_r = None\n",
    "            self.bias_h = None\n",
    "        super(SoftAttnGRU, self).build(input_shape)\n",
    "\n",
    "    def step(self, inputs, states, training=None):\n",
    "        x_i, attn_gate = array_ops.split(inputs,\n",
    "                                         num_or_size_splits=[self.units, 1], axis=1)\n",
    "        h_tm1 = states[0]\n",
    "\n",
    "        dp_mask = self._dropout_mask\n",
    "        rec_dp_mask = self._recurrent_dropout_mask\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                inputs_z = x_i * dp_mask[0]\n",
    "                inputs_r = x_i * dp_mask[1]\n",
    "                inputs_h = x_i * dp_mask[2]\n",
    "            else:\n",
    "                inputs_z = x_i\n",
    "                inputs_r = x_i\n",
    "                inputs_h = x_i\n",
    "            x_z = K.dot(inputs_z, self.kernel_z)\n",
    "            x_r = K.dot(inputs_r, self.kernel_r)\n",
    "            x_h = K.dot(inputs_h, self.kernel_h)\n",
    "            if self.use_bias:\n",
    "                x_z = K.bias_add(x_z, self.bias_z)\n",
    "                x_r = K.bias_add(x_r, self.bias_r)\n",
    "                x_h = K.bias_add(x_h, self.bias_h)\n",
    "\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1_z = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_r = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_h = h_tm1 * rec_dp_mask[2]\n",
    "            else:\n",
    "                h_tm1_z = h_tm1\n",
    "                h_tm1_r = h_tm1\n",
    "                h_tm1_h = h_tm1\n",
    "\n",
    "            z = self.recurrent_activation(\n",
    "                x_z + K.dot(h_tm1_z, self.recurrent_kernel_z))\n",
    "            r = self.recurrent_activation(\n",
    "                x_r + K.dot(h_tm1_r, self.recurrent_kernel_r))\n",
    "\n",
    "            hh = self.activation(x_h + K.dot(r * h_tm1_h,\n",
    "                                             self.recurrent_kernel_h))\n",
    "        else:\n",
    "            if 0. < self.dropout < 1.:\n",
    "                x_i *= dp_mask[0]\n",
    "            matrix_x = K.dot(x_i, self.kernel)\n",
    "            if self.use_bias:\n",
    "                matrix_x = K.bias_add(matrix_x, self.bias)\n",
    "            if 0. < self.recurrent_dropout < 1.:\n",
    "                h_tm1 *= rec_dp_mask[0]\n",
    "            matrix_inner = K.dot(h_tm1,\n",
    "                                 self.recurrent_kernel[:, :2 * self.units])\n",
    "\n",
    "            x_z = matrix_x[:, :self.units]\n",
    "            x_r = matrix_x[:, self.units: 2 * self.units]\n",
    "            recurrent_z = matrix_inner[:, :self.units]\n",
    "            recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n",
    "\n",
    "            z = self.recurrent_activation(x_z + recurrent_z)\n",
    "            r = self.recurrent_activation(x_r + recurrent_r)\n",
    "\n",
    "            x_h = matrix_x[:, 2 * self.units:]\n",
    "            recurrent_h = K.dot(r * h_tm1,\n",
    "                                self.recurrent_kernel[:, 2 * self.units:])\n",
    "            hh = self.activation(x_h + recurrent_h)\n",
    "        h = z * h_tm1 + (1 - z) * hh\n",
    "        h = attn_gate * h + (1 - attn_gate) * h_tm1\n",
    "\n",
    "        if 0 < self.dropout + self.recurrent_dropout:\n",
    "            if training is None:\n",
    "                h._uses_learning_phase = True\n",
    "        return h, [h]\n",
    "\n",
    "    def call(self, input_list, initial_state=None, mask=None, training=None):\n",
    "\n",
    "        inputs = input_list\n",
    "\n",
    "        self._generate_dropout_mask(inputs, training=training)\n",
    "        self._generate_recurrent_dropout_mask(inputs, training=training)\n",
    "        self.training = training\n",
    "        uses_learning_phase = False\n",
    "        initial_state = self.get_initial_state(inputs)\n",
    "\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        last_output, outputs, _ = K.rnn(self.step,\n",
    "                                        inputs=inputs,\n",
    "                                        constants=[],\n",
    "                                        initial_states=initial_state,\n",
    "                                        input_length=input_shape[1],\n",
    "                                        unroll=False)\n",
    "        if self.return_sequences:\n",
    "            y = outputs\n",
    "        else:\n",
    "            y = last_output\n",
    "\n",
    "        if uses_learning_phase:\n",
    "            y._uses_learning_phase = True\n",
    "\n",
    "        if self.return_sequences:\n",
    "            timesteps = input_shape[1]\n",
    "            new_time_steps = list(y.get_shape())\n",
    "            new_time_steps[1] = timesteps\n",
    "            y.set_shape(new_time_steps)\n",
    "        return y\n",
    "\n",
    "    def _generate_dropout_mask(self, inputs, training=None):\n",
    "        if 0 < self.dropout < 1:\n",
    "            ones = K.ones_like(K.squeeze(inputs[:, 0:1, :-1], axis=1))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            self._dropout_mask = [K.in_train_phase(\n",
    "                dropped_inputs,\n",
    "                ones,\n",
    "                training=training)\n",
    "                for _ in range(3)]\n",
    "        else:\n",
    "            self._dropout_mask = None\n",
    "\n",
    "    def _generate_recurrent_dropout_mask(self, inputs, training=None):\n",
    "        if 0 < self.recurrent_dropout < 1:\n",
    "            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))\n",
    "            ones = K.tile(ones, (1, self.units))\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(ones, self.dropout)\n",
    "\n",
    "            self._recurrent_dropout_mask = [K.in_train_phase(\n",
    "                dropped_inputs,\n",
    "                ones,\n",
    "                training=training)\n",
    "                for _ in range(3)]\n",
    "        else:\n",
    "            self._recurrent_dropout_mask = None\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        initial_state = K.zeros_like(inputs)\n",
    "        initial_state = initial_state[:, :, :-1]\n",
    "        initial_state = K.sum(initial_state, axis=(1, 2))\n",
    "        initial_state = K.expand_dims(initial_state)\n",
    "        if hasattr(self.state_size, '__len__'):\n",
    "            return [K.tile(initial_state, [1, dim])\n",
    "                    for dim in self.state_size]\n",
    "        else:\n",
    "            return [K.tile(initial_state, [1, self.state_size])]\n",
    "          \n",
    "\n",
    "class EpisodicMemoryModule(Layer):\n",
    "\n",
    "    def __init__(self, units,  emb_dim,\n",
    "                 batch_size, memory_steps=3, dropout=0.0, **kwargs):\n",
    "        self.memory_steps = memory_steps\n",
    "        self.dropout = dropout\n",
    "        self.name = \"episodic_memory_module\"\n",
    "        self._input_map = {}\n",
    "        self.supports_masking = True\n",
    "        self.units = units\n",
    "\n",
    "        # attention net.\n",
    "        self.l_1 = Dense(units=emb_dim,\n",
    "                         batch_size=batch_size,\n",
    "                         activation='tanh')\n",
    "\n",
    "        self.l_2 = Dense(units=1,\n",
    "                         batch_size=batch_size,\n",
    "                         activation=None)\n",
    "\n",
    "        # Episode net\n",
    "        self.episode_GRU = SoftAttnGRU(units=units,\n",
    "                                       return_sequences=False,\n",
    "                                       batch_size=batch_size)\n",
    "\n",
    "        # Memory generating network\n",
    "        self.memory_net = Dense(units=units,\n",
    "                                activation='relu')\n",
    "\n",
    "        super(EpisodicMemoryModule, self).__init__()\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        q_shape = list(input_shape[1])\n",
    "        q_shape[-1] = self.units * 2\n",
    "\n",
    "        return tuple(q_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(EpisodicMemoryModule, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def compute_attention(fact, question, memory):\n",
    "            f_i = [\n",
    "                fact * question,\n",
    "                fact * memory,\n",
    "                K.abs(\n",
    "                    fact - question),\n",
    "                K.abs(\n",
    "                    fact - memory)]\n",
    "            g_t_i = self.l_1(K.concatenate(f_i, axis=1))\n",
    "            g_t_i = self.l_2(g_t_i)\n",
    "            return g_t_i\n",
    "\n",
    "        facts = inputs[0]\n",
    "        question = inputs[1]\n",
    "        memory = K.identity(question) \n",
    "        fact_list = tf.unstack(facts, axis=1)\n",
    "\n",
    "        for step in range(self.memory_steps):\n",
    "            attentions = [tf.squeeze(compute_attention(fact, question, memory), axis=1)\n",
    "                for i, fact in enumerate(fact_list)]\n",
    "\n",
    "        attentions = tf.stack(attentions)\n",
    "        attentions = tf.transpose(attentions)\n",
    "        attentions = tf.nn.softmax(attentions)\n",
    "        attentions = tf.expand_dims(attentions, axis=-1)\n",
    "\n",
    "        episode = K.concatenate([facts, attentions], axis=2)\n",
    "        episode = self.episode_GRU(episode)\n",
    "\n",
    "        memory = self.memory_net(K.concatenate([memory, episode, question], axis=1))\n",
    "\n",
    "        return K.concatenate([memory, question], axis=1)    \n",
    "      \n",
    "      \n",
    "class DynamicMemoryNetwork():\n",
    "\n",
    "    def __init__(self, save_folder):\n",
    "        self.save_folder = save_folder\n",
    "        self.model_path = os.path.join(save_folder, \"dmn-{epoch:02d}\")\n",
    "        self.log_folder = os.path.join(save_folder, \"log\")\n",
    "        if not os.path.exists(self.log_folder):\n",
    "            os.makedirs(self.log_folder)\n",
    "\n",
    "    def fit(self,\n",
    "            train_x,\n",
    "            train_q,\n",
    "            train_y,\n",
    "            epochs=256,\n",
    "            validation_split=0.15,\n",
    "            l_rate=1e-3,\n",
    "            l_decay=0,\n",
    "            save_criteria='val_loss',\n",
    "            save_criteria_mode='min'\n",
    "            ):\n",
    "\n",
    "        opt = optimizers.Adam(lr=l_rate, decay=l_decay, clipvalue=10.)\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(self.model_path,\n",
    "                                                     monitor=save_criteria,\n",
    "                                                     verbose=1,\n",
    "                                                     save_best_only=True,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     mode=save_criteria_mode,\n",
    "                                                     period=1)\n",
    "\n",
    "        logger = keras.callbacks.CSVLogger(\n",
    "            os.path.join(\n",
    "                self.log_folder,\n",
    "                \"log.csv\"),\n",
    "            separator=',',\n",
    "            append=False)\n",
    "\n",
    "        stopper = keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                                mode='min',\n",
    "                                                patience=25,\n",
    "                                                min_delta=1e-4\n",
    "                                                )\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"categorical_accuracy\"])\n",
    "        print('Metrics: {self.model.metrics_names}')\n",
    "        train_history = self.model.fit(x={'input_tensor': train_x,\n",
    "                                          'question_tensor': train_q},\n",
    "                                       y=train_y,\n",
    "                                       callbacks=[logger, checkpoint, stopper],\n",
    "                                       batch_size=self.batch_size,\n",
    "                                       validation_split=validation_split,\n",
    "                                       epochs=epochs)\n",
    "        self.model.save_weights(self.model_path + \"_trained\")\n",
    "        return train_history\n",
    "\n",
    "    def validate_model(self, x_val, xq_val, y_val):\n",
    "        loss, acc = model.evaluate([x_val, xq_val], y_val,\n",
    "                                   batch_size=self.batch_size)\n",
    "        return loss, acc\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, x, xq, batch_size=1):\n",
    "        return self.model.predict([x, xq], batch_size=batch_size)\n",
    "\n",
    "    def build_inference_graph(self, input_shape, question_shape, num_classes,\n",
    "                              units=256,batch_size=32, memory_steps=3, dropout=0.1):\n",
    "        emb_dim = input_shape[-1]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        inputs_tensor = Input(\n",
    "            batch_shape=(\n",
    "                batch_size,\n",
    "            ) + input_shape,\n",
    "            name='input_tensor')\n",
    "\n",
    "        question_tensor = Input(\n",
    "            batch_shape=(\n",
    "                batch_size,\n",
    "            ) + question_shape,\n",
    "            name='question_tensor')\n",
    "\n",
    "        gru_layer = GRU(units=units,\n",
    "                        dropout=dropout,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "        facts = Bidirectional(gru_layer, merge_mode='sum')(inputs_tensor)\n",
    "        facts = Dropout(dropout)(facts)\n",
    "\n",
    "        facts_shape = list(K.int_shape(facts))\n",
    "        facts_shape[1] = input_shape[0]\n",
    "        facts.set_shape(facts_shape)\n",
    "\n",
    "        question = GRU(units=units, stateful=True, return_sequences=False,\n",
    "                       batch_size=batch_size)(question_tensor)\n",
    "\n",
    "        answer = EpisodicMemoryModule(\n",
    "            units=units,\n",
    "            batch_size=batch_size,\n",
    "            emb_dim=emb_dim,\n",
    "            memory_steps=memory_steps)([facts, question])\n",
    "\n",
    "        answer = Dropout(dropout)(answer)\n",
    "\n",
    "        answer = Dense(\n",
    "            units=num_classes,\n",
    "            batch_size=batch_size,\n",
    "            activation=\"softmax\")(answer)\n",
    "\n",
    "        self.model = Model(\n",
    "            inputs=[\n",
    "                inputs_tensor,\n",
    "                question_tensor],\n",
    "            outputs=answer)\n",
    "        \n",
    "\n",
    "# Download glove embeddings\n",
    "\n",
    "\n",
    "# Run Experiments\n",
    "\n",
    "if TASK == 1:\n",
    "  max_len, trainset, testset = load_dataset(emb_location=\"./glove.6B.50d.txt\",\n",
    "                                            babi_location=\"./tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt\",\n",
    "                                            babi_test_location=\"./tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_test.txt\",\n",
    "                                            emb_dim=50)\n",
    "elif TASK == 2:\n",
    "  max_len, trainset, testset = load_dataset(emb_location=\"./glove.6B.50d.txt\",\n",
    "                                            babi_location=\"./tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_train.txt\",\n",
    "                                            babi_test_location=\"./tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_test.txt\",\n",
    "                                            emb_dim=50)\n",
    "else:\n",
    "  max_len, trainset, testset = load_dataset(emb_location=\"./glove.6B.50d.txt\",\n",
    "                                            babi_location=\"./tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_train.txt\",\n",
    "                                            babi_test_location=\"./tasks_1-20_v1-2/en-10k/qa1_three-supporting-facts_test.txt\",\n",
    "                                            emb_dim=50)\n",
    "  \n",
    "  \n",
    "input_shape = trainset[0][0].shape\n",
    "question_shape = trainset[1][0].shape\n",
    "num_classes = len(trainset[2][0])\n",
    "\n",
    "print(\"Dataset Loaded. Compiling Model...\")\n",
    "dmn_net = DynamicMemoryNetwork(save_folder=\"./dmn\")\n",
    "dmn_net.build_inference_graph(\n",
    "    input_shape=input_shape,\n",
    "    question_shape=question_shape,\n",
    "    num_classes=num_classes,\n",
    "    units=HIDDEN_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    memory_steps=MEM_STEPS,\n",
    "    dropout=0.3)\n",
    "\n",
    "print(\"Model Compiled. Training...\")\n",
    "\n",
    "dmn_net.fit(trainset[0], trainset[1], trainset[2],\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.05,\n",
    "            l_rate= 0.001,\n",
    "            l_decay=0)\n",
    "\n",
    "if testset is not None:\n",
    "    print(\"Model Trained. Evaluating...\")\n",
    "    loss, acc = dmn_net.model.evaluate(x=[testset[0], testset[1]],y=testset[2], batch_size=50)\n",
    "    print('Test Loss: {loss}, Test Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6THZMfJvrwj"
   },
   "source": [
    "## Differentiable Neural Computer\n",
    "\n",
    "The differentiable neural computer (DNC) is a neural network with an independent memory bank. It is an embedded neural network controller with a collection of preset operations for memory storage and management. As an extension of the neural Turing machine architecture, it allows for scaling of memory without having to scale the rest of the network.\n",
    "\n",
    "The heart of a DNC is a neural network called a controller, which is analogous to a CPU in a computer. This DNC controller can perform several operations on memory concurrently, including reading and writing to multiple memory locations at once and producing output predictions. As before, the memory is a set of locations that can each store a vector of information. The DNC controller can use soft attention to search memory based on the content of each location, or associative temporal links can be traversed forward or backward to recall sequence information in either direction. Queried information can then be used for prediction.\n",
    "\n",
    "For this case study, we will apply the TensorFlow-DNC implementation (https://github.com/bgavran/DNC) originally developed by DeepMind (https://github.com/deepmind/dnc) to the bAbI extended datasets. We train a DNC model using the extended bAbI training sets with a hidden size of 256, memory size of 256, 4 read heads, 1 write head, batch size of 1, and the RMSprop optimizer with gradient clipping for 20,000 iterations.  It may not surprising to see that the DNC model outperforms all previous models, given the increased complexity. The trade off between accuracy and training time should be carefully weighed when choosing which architecture is most suitable for the task. For simple tasks, a single LSTM implementation may be all that is required. DNCs with their scalable memory are a better choice when complex knowledge is required for task prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VPZkruFyckgu",
    "outputId": "6d9b5853-6dad-45b9-c722-7ec1304a8d2e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import timeline\n",
    "from natsort import natsorted\n",
    "\n",
    "class Task:\n",
    "\n",
    "    def generate_data(self, train=True, cost=9999):\n",
    "        pass\n",
    "\n",
    "    def cost(self, network_output, correct_output, mask=None):\n",
    "        pass\n",
    "\n",
    "    def test(self, sess, outputs_tf, fd, batch_size):\n",
    "        pass\n",
    "      \n",
    "\n",
    "class bAbITask(Task):\n",
    "    base = \".\"\n",
    "    output_symbol = \"-\"\n",
    "    pad_symbol = \"*\"\n",
    "    newstory_delimiter = \" NEWSTORY \"\n",
    "    processed_append = \"-processed.p\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Init tries to read from pickle file, if it doesn't exist it creates it.\n",
    "        \"\"\"\n",
    "        self.tasks_dir = os.path.join(\"tasks_1-20_v1-2\", \"en-10k\")\n",
    "        print(self.tasks_dir)\n",
    "        self.processed_dir = self.tasks_dir + bAbITask.processed_append\n",
    "        self.files_path = []\n",
    "\n",
    "        for f in os.listdir(self.tasks_dir):\n",
    "            f_path = os.path.join(self.tasks_dir, f)\n",
    "            if os.path.isfile(f_path):\n",
    "                self.files_path.append(f_path)\n",
    "\n",
    "        if not os.path.isfile(self.processed_dir):\n",
    "            pickle.dump(self.preprocess_files(), open(self.processed_dir, \"wb\"))\n",
    "            print(\"Pickled!\", self.processed_dir)\n",
    "\n",
    "        self.word_to_ind, all_input_stories, all_output_stories = pickle.load(open(self.processed_dir, \"rb\"))\n",
    "        self.ind_to_word = {ind: word for word, ind in self.word_to_ind.items()}\n",
    "\n",
    "        self.train_list = natsorted([k for k, v in all_input_stories.items() if k[-9:] == \"train.txt\"])\n",
    "        self.test_list = natsorted([k for k, v in all_input_stories.items() if k[-8:] == \"test.txt\"])\n",
    "\n",
    "        self.vector_size = len(self.word_to_ind)\n",
    "        self.n_tasks = 3\n",
    "\n",
    "        self.x_train_stories = {k: v for k, v in all_input_stories.items() if k in self.train_list}\n",
    "        self.y_train_stories = {k: v for k, v in all_output_stories.items() if k in self.train_list}\n",
    "\n",
    "        self.x_test_stories = {k: v for k, v in all_input_stories.items() if k in self.test_list}\n",
    "        self.y_test_stories = {k: v for k, v in all_output_stories.items() if k in self.test_list}\n",
    "\n",
    "        assert len(self.x_train_stories.keys()) == len(self.y_train_stories.keys())\n",
    "\n",
    "        self.x_shape = [None, None, self.vector_size]\n",
    "        self.y_shape = [None, None, self.vector_size]\n",
    "        self.mask = [None, None, 1]\n",
    "\n",
    "        self.mean_test_errors = []\n",
    "\n",
    "    def display_output(self, prediction, data_batch, mask):\n",
    "        \"\"\"\n",
    "        For a batch of stories and the corresponding network output, it prints the first story and its output.\n",
    "        \"\"\"\n",
    "        prediction = prediction[:1, :, :]\n",
    "        mask = mask[:1, :, :]\n",
    "        data_batch = data_batch[:, :1, :, :]\n",
    "\n",
    "        text = self.indices_to_words([np.argmax(i) for i in data_batch[0][0]])\n",
    "\n",
    "        correct_indices = bAbITask.tensor_to_indices(data_batch[1], mask)\n",
    "        out_indices = bAbITask.tensor_to_indices(prediction, mask)\n",
    "\n",
    "        correct_words = self.indices_to_words(correct_indices)\n",
    "        out_words = self.indices_to_words(out_indices)\n",
    "\n",
    "        print(text)\n",
    "        print(\"Output:\", out_words)\n",
    "        print(\"Correct:\", correct_words)\n",
    "        print(\"-------------------------------------------------------------------\\n\")\n",
    "\n",
    "    def indices_to_words(self, indices):\n",
    "        return \" \".join([self.ind_to_word[ind] for ind in indices])\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_to_indices(data_tensor, mask):\n",
    "        \"\"\"\n",
    "        Converts the one hot tensor to indices\n",
    "        \"\"\"\n",
    "        assert len(data_tensor.shape) == 3 and data_tensor.shape[0] == mask.shape[0]\n",
    "        locations = np.unique(np.nonzero(data_tensor * mask)[1])\n",
    "        indices = np.argmax(data_tensor[0, locations, :], axis=1)\n",
    "        return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def to_onehot(x, depth):\n",
    "        return np.array([np.eye(depth)[int(indices)] for indices in x])\n",
    "\n",
    "    def generate_data(self, batch_size=16, train=True, cost=None):\n",
    "        \"\"\"\n",
    "        Main method for generating train/test data.\n",
    "        \"\"\"\n",
    "        task_indices = np.random.randint(0, self.n_tasks, batch_size)\n",
    "        if train:\n",
    "            task_names = [self.train_list[ind] for ind in task_indices]\n",
    "            x_task_names_stories = [self.x_train_stories[task_name] for task_name in task_names]\n",
    "            y_task_names_stories = [self.y_train_stories[task_name] for task_name in task_names]\n",
    "        else:\n",
    "            task_names = [self.test_list[ind] for ind in task_indices]\n",
    "            x_task_names_stories = [self.x_test_stories[task_name] for task_name in task_names]\n",
    "            y_task_names_stories = [self.y_test_stories[task_name] for task_name in task_names]\n",
    "        x = []\n",
    "        y = []\n",
    "        for x_task_stories, y_task_stories in zip(x_task_names_stories, y_task_names_stories):\n",
    "            story_ind = np.random.randint(0, len(x_task_stories))\n",
    "            x.append(bAbITask.to_onehot(x_task_stories[story_ind], self.vector_size))\n",
    "            y.append(bAbITask.to_onehot(y_task_stories[story_ind], self.vector_size))\n",
    "\n",
    "        x, y, lengths = self.pad_stories(x, y)\n",
    "        return np.array([x, y]), lengths, x[:, :, :1]\n",
    "\n",
    "    def pad_stories(self, x, y):\n",
    "        \"\"\"\n",
    "        Pads the stories in a batch to the size of the longest one\n",
    "        \"\"\"\n",
    "        lengths = [len(story) for story in x]\n",
    "        max_length = np.max(lengths)\n",
    "\n",
    "        for i, story in enumerate(x):\n",
    "            padding = bAbITask.to_onehot(np.ones(max_length - len(story)), self.vector_size)\n",
    "            if len(padding) > 0:\n",
    "                x[i] = np.vstack((x[i], padding))\n",
    "                y[i] = np.vstack((y[i], padding))\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        return x, y, lengths\n",
    "\n",
    "    def test(self, sess, outputs_tf, fd, batch_size):\n",
    "        \"\"\"\n",
    "        Evaluates the performance of the network on the whole test set.\n",
    "        \"\"\"\n",
    "        from time import time\n",
    "        t = time()\n",
    "        print(\"Testing...\")\n",
    "        num_passed_tasks = 0\n",
    "        num_tasks = len(self.x_test_stories)\n",
    "        task_errors = []\n",
    "        for ind, (inp, output) in enumerate(zip(self.x_test_stories.items(), self.y_test_stories.items())):\n",
    "            correct_questions = 0\n",
    "            total_questions = 0\n",
    "            num_stories = len(inp[1])\n",
    "            x = [bAbITask.to_onehot(i, self.vector_size) for i in inp[1][:batch_size]]\n",
    "            y = [bAbITask.to_onehot(i, self.vector_size) for i in output[1][:batch_size]]\n",
    "            for index in range(num_stories):\n",
    "                x = list(x)\n",
    "                y = list(y)\n",
    "                x[0] = bAbITask.to_onehot(inp[1][index], self.vector_size)\n",
    "                y[0] = bAbITask.to_onehot(output[1][index], self.vector_size)\n",
    "                x, y, lengths = self.pad_stories(x, y)\n",
    "                m = x[:, :, :1]\n",
    "                outputs = sess.run(outputs_tf, feed_dict={fd[0]: x, fd[1]: y, fd[2]: lengths, fd[3]: m})\n",
    "\n",
    "                outputs_list = bAbITask.tensor_to_indices(outputs[:1], m[:1])\n",
    "                correct_list = bAbITask.tensor_to_indices(y[:1], m[:1])\n",
    "                answers = y[0] * m[0, :, :1]\n",
    "                locations = np.argwhere(answers > 0)[:, 0]\n",
    "                i = 0\n",
    "                while i < len(locations):\n",
    "                    all_words_correct = True\n",
    "                    j = 0\n",
    "                    while i + j < len(locations) and locations[i] + j == locations[i + j]:\n",
    "                        if outputs_list[i + j] != correct_list[i + j]:\n",
    "                            all_words_correct = False\n",
    "                        j += 1\n",
    "                    total_questions += 1\n",
    "                    correct_questions += all_words_correct\n",
    "                    i += j\n",
    "\n",
    "            task_name = inp[0].split(\"/\")[-1]\n",
    "            task_error = 1 - correct_questions / total_questions\n",
    "            print(ind, task_name, \" Total_correct:\", correct_questions, \" total questions:\", total_questions,\n",
    "                  \" task error:\", task_error * 100, \"%\")\n",
    "            num_passed_tasks += task_error <= 0.05\n",
    "            task_errors.append(task_error)\n",
    "        mean_error = np.mean(task_errors)\n",
    "        print(\"TOTAL PASSED TASKS:\", num_passed_tasks, \"TOTAL TASKS:\", num_tasks, \" MEAN ERROR\", mean_error)\n",
    "        self.mean_test_errors.append(mean_error)\n",
    "        print(\"ALL ERRORS: \", self.mean_test_errors)\n",
    "        print(\"Time == \", time() - t)\n",
    "\n",
    "    def cost(self, network_output, correct_output, mask=None):\n",
    "\n",
    "        softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=network_output,\n",
    "                                                                        labels=correct_output,\n",
    "                                                                        dim=2)\n",
    "        masked = softmax_cross_entropy * mask[:, :, 0]\n",
    "        tf.summary.image(\"0Masked_SCE\", tf.reshape(masked, [1, 1, -1, 1]))\n",
    "        return tf.reduce_mean(masked)\n",
    "\n",
    "    def preprocess_files(self):\n",
    "        word_to_ind = {bAbITask.output_symbol: 0, bAbITask.pad_symbol: 1}\n",
    "        all_input_stories, all_output_stories = dict(), dict()\n",
    "        for file_path in self.files_path:\n",
    "            print(file_path)\n",
    "            file = open(file_path).read().lower()\n",
    "            file = re.sub(\"\\n1 \", bAbITask.newstory_delimiter, file)  # adding a delimeter between two stories\n",
    "            file = re.sub(\"\\d+|\\n|\\t\", \" \", file)  # removing all numbers, newlines and tabs\n",
    "            file = re.sub(\"([?.])\", r\" \\1\", file)  # adding a space before all punctuations\n",
    "            stories = file.split(bAbITask.newstory_delimiter)\n",
    "\n",
    "            input_stories = []\n",
    "            output_stories = []\n",
    "            for i, story in enumerate(stories):\n",
    "                input_tokens = story.split()\n",
    "                output_tokens = story.split()\n",
    "\n",
    "                for i, token in enumerate(input_tokens):\n",
    "                    if token == \"?\":\n",
    "                        output_tokens[i + 1] = output_tokens[i + 1].split(\",\")\n",
    "                        input_tokens[i + 1] = [bAbITask.output_symbol for _ in range(len(output_tokens[i + 1]))]\n",
    "\n",
    "                input_tokens = bAbITask.flatten_if_list(input_tokens)\n",
    "                output_tokens = bAbITask.flatten_if_list(output_tokens)\n",
    "\n",
    "                for token in output_tokens:\n",
    "                    if token not in word_to_ind:\n",
    "                        word_to_ind[token] = len(word_to_ind)\n",
    "\n",
    "                input_stories.append([word_to_ind[elem] for elem in input_tokens])\n",
    "                output_stories.append([word_to_ind[elem] for elem in output_tokens])\n",
    "            all_input_stories[file_path] = input_stories\n",
    "            all_output_stories[file_path] = output_stories\n",
    "        return word_to_ind, all_input_stories, all_output_stories\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_if_list(l):\n",
    "        newl = []\n",
    "        for elem in l:\n",
    "            if isinstance(elem, list):\n",
    "                newl.extend(elem)\n",
    "            else:\n",
    "                newl.append(elem)\n",
    "        return newl\n",
    "\n",
    "      \n",
    "class ProjectPath:\n",
    "    base = \".\"\n",
    "\n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        from time import localtime, strftime\n",
    "        self.timestamp = strftime(\"%B_%d__%H:%M\", localtime())\n",
    "\n",
    "        self.log_path = os.path.join(ProjectPath.base, self.log_dir, self.timestamp)\n",
    "        self.train_path = os.path.join(self.log_path, \"train\")\n",
    "        self.test_path = os.path.join(self.log_path, \"test\")\n",
    "        self.model_path = os.path.join(self.train_path, \"model.chpt\")\n",
    "\n",
    "\n",
    "def init_wrapper(init_fn):\n",
    "    def inner(shape, stddev):\n",
    "        if stddev is None:\n",
    "            return init_fn(shape)\n",
    "        else:\n",
    "            return init_fn(shape, stddev=stddev)\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "#### Memory #############################################  \n",
    "\n",
    "class Memory:\n",
    "\n",
    "    epsilon = 1e-6\n",
    "    max_outputs = 2\n",
    "\n",
    "    def __init__(self, batch_size, controller_output_size, out_vector_size, mem_hp, initializer=tf.random_normal,\n",
    "                 initial_stddev=0.1):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.controller_output_size = controller_output_size\n",
    "        self.out_vector_size = out_vector_size\n",
    "\n",
    "        self.memory_size = mem_hp.mem_size\n",
    "        self.word_size = mem_hp.word_size\n",
    "        self.num_read_heads = mem_hp.num_read_heads\n",
    "\n",
    "        self.interface_vector_size = (self.word_size * self.num_read_heads) + \\\n",
    "                                     5 * self.num_read_heads + 3 * self.word_size + 3\n",
    "\n",
    "        self.interface_weights = tf.Variable(\n",
    "            initializer([self.controller_output_size, self.interface_vector_size], stddev=initial_stddev),\n",
    "            name=\"interface_weights\")\n",
    "        self.output_weights = tf.Variable(\n",
    "            initializer([self.num_read_heads, self.word_size, self.out_vector_size], stddev=initial_stddev),\n",
    "            name=\"output_weights_memory\")\n",
    "\n",
    "        r, w = self.num_read_heads, self.word_size\n",
    "        sizes = [r * w, r, w, 1, w, w, r, 1, 1, 3 * r]\n",
    "        names = [\"r_read_keys\", \"r_read_strengths\", \"write_key\", \"write_strength\", \"erase_vector\", \"write_vector\",\n",
    "                 \"r_free_gates\", \"allocation_gate\", \"write_gate\", \"r_read_modes\"]\n",
    "        functions = [tf.identity, Memory.oneplus, tf.identity, Memory.oneplus, tf.nn.sigmoid, tf.identity,\n",
    "                     tf.nn.sigmoid, tf.nn.sigmoid, tf.nn.sigmoid, self.reshape_and_softmax]\n",
    "\n",
    "        indexes = [[sum(sizes[:i]), sum(sizes[:i + 1])] for i in range(len(sizes))]\n",
    "        assert len(names) == len(sizes) == len(functions) == len(indexes)\n",
    "        self.split_interface = lambda iv: {name: fn(iv[:, i[0]:i[1]]) for name, i, fn in\n",
    "                                           zip(names, indexes, functions)}\n",
    "\n",
    "    def init_memory(self, batch_size):\n",
    "        read_weightings = tf.fill([batch_size, self.memory_size, self.num_read_heads], Memory.epsilon)\n",
    "        write_weighting = tf.fill([batch_size, self.memory_size], Memory.epsilon, name=\"Write_weighting\")\n",
    "        precedence_weighting = tf.zeros([batch_size, self.memory_size], name=\"Precedence_weighting\")\n",
    "        m = tf.fill([batch_size, self.memory_size, self.word_size], Memory.epsilon)  # initial memory matrix\n",
    "        usage_vector = tf.zeros([batch_size, self.memory_size], name=\"Usage_vector\")\n",
    "        link_matrix = tf.zeros([batch_size, self.memory_size, self.memory_size])\n",
    "        read_vectors = tf.fill([batch_size, self.num_read_heads, self.word_size], Memory.epsilon)\n",
    "\n",
    "        return [read_weightings, write_weighting, usage_vector, precedence_weighting, m, link_matrix, read_vectors]\n",
    "\n",
    "    def step(self, controller_output, memory_state):\n",
    "        read_weightings, write_weighting, usage_vector, precedence_weighting, m, link_matrix, read_vectors = \\\n",
    "            memory_state\n",
    "\n",
    "        interface_vector = controller_output @ self.interface_weights  # shape [batch_size, interf_vector_size]\n",
    "\n",
    "        interf = self.split_interface(interface_vector)\n",
    "\n",
    "        interf[\"write_key\"] = tf.expand_dims(interf[\"write_key\"], dim=1)\n",
    "        interf[\"r_read_keys\"] = tf.reshape(interf[\"r_read_keys\"], [-1, self.num_read_heads, self.word_size])\n",
    "\n",
    "        memory_retention = tf.reduce_prod(1 - tf.einsum(\"br,bnr->bnr\", interf[\"r_free_gates\"], read_weightings), 2)\n",
    "\n",
    "        usage_vector = (usage_vector + write_weighting - usage_vector * write_weighting) * memory_retention\n",
    "        allocation_weighting = self.calculate_allocation_weighting(usage_vector)\n",
    "\n",
    "        write_content_weighting = Memory.content_based_addressing(m, interf[\"write_key\"], interf[\"write_strength\"])\n",
    "\n",
    "        write_weighting = interf[\"write_gate\"] * (interf[\"allocation_gate\"] * allocation_weighting\n",
    "                                                  + tf.einsum(\"bnr,bi->bn\",\n",
    "                                                              write_content_weighting,\n",
    "                                                              (1 - interf[\"allocation_gate\"])))\n",
    "\n",
    "        m = m * (1 - tf.einsum(\"bn,bw->bnw\", write_weighting, interf[\"erase_vector\"])) + \\\n",
    "            tf.einsum(\"bn,bw->bnw\", write_weighting, interf[\"write_vector\"])\n",
    "\n",
    "        link_matrix = self.update_link_matrix(link_matrix, precedence_weighting, write_weighting)\n",
    "        precedence_weighting = tf.einsum(\"bn,b->bn\",\n",
    "                                         precedence_weighting,\n",
    "                                         (1 - tf.reduce_sum(write_weighting, axis=1))) + write_weighting\n",
    "\n",
    "        forwardw = tf.einsum(\"bmn,bnr->bmr\", link_matrix, read_weightings)\n",
    "        backwardw = tf.einsum(\"bnm,bnr->bmr\", link_matrix, read_weightings)\n",
    "\n",
    "        read_content_weighting = Memory.content_based_addressing(m, interf[\"r_read_keys\"],\n",
    "                                                                 interf[\"r_read_strengths\"])\n",
    "\n",
    "        read_weightings = Memory.calculate_read_weightings(interf[\"r_read_modes\"],\n",
    "                                                           backwardw,\n",
    "                                                           read_content_weighting,\n",
    "                                                           forwardw)\n",
    "\n",
    "        read_vectors = tf.einsum(\"bnw,bnr->brw\", m, read_weightings)\n",
    "\n",
    "        memory_output = tf.einsum(\"brw,rwo->bo\", read_vectors, self.output_weights)\n",
    "\n",
    "        extra_visualization_info = [interf[\"r_read_modes\"], interf[\"write_gate\"], interf[\"allocation_gate\"],\n",
    "                                    interf[\"write_strength\"], interf[\"r_read_strengths\"], interf[\"erase_vector\"],\n",
    "                                    interf[\"write_vector\"], forwardw, backwardw, interf[\"r_free_gates\"],\n",
    "                                    interf[\"r_read_keys\"]]\n",
    "\n",
    "        memory_state = [read_weightings,\n",
    "                        write_weighting,\n",
    "                        usage_vector,\n",
    "                        precedence_weighting,\n",
    "                        m,\n",
    "                        link_matrix,\n",
    "                        read_vectors]\n",
    "        return memory_output, memory_state, extra_visualization_info\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_read_weightings(r_read_modes, backwardw, read_content_weighting, forwardw):\n",
    "        return tf.einsum(\"brs,bnrs->bnr\", r_read_modes, tf.stack([backwardw, read_content_weighting, forwardw], axis=3))\n",
    "\n",
    "    def calculate_allocation_weighting(self, usage_vector):\n",
    "        usage_vector = Memory.epsilon + (1 - Memory.epsilon) * usage_vector\n",
    "\n",
    "        highest_usage, inverse_indices = tf.nn.top_k(-usage_vector, k=self.memory_size)\n",
    "        lowest_usage = -highest_usage\n",
    "\n",
    "        allocation_scrambled = (1 - lowest_usage) * tf.cumprod(lowest_usage, axis=1, exclusive=True)\n",
    "\n",
    "        indices = tf.stack([tf.invert_permutation(batch_indices) for batch_indices in tf.unstack(inverse_indices)])\n",
    "        allocation = tf.stack([tf.gather(mem, ind)\n",
    "                               for mem, ind in\n",
    "                               zip(tf.unstack(allocation_scrambled), tf.unstack(indices))])\n",
    "\n",
    "        return allocation\n",
    "\n",
    "    def update_link_matrix(self, link_matrix_old, precedence_weighting_old, write_weighting):\n",
    "        expanded = tf.expand_dims(write_weighting, axis=2)\n",
    "\n",
    "        w = tf.tile(expanded, [1, 1, self.memory_size])  # shape [batch_size, memory_size, memory_size]\n",
    "        w_transp = tf.tile(tf.transpose(expanded, [0, 2, 1]), [1, self.memory_size, 1])\n",
    "\n",
    "        lm = (1 - w - w_transp) * link_matrix_old + tf.einsum(\"bn,bm->bmn\", precedence_weighting_old, write_weighting)\n",
    "        lm *= (1 - tf.eye(self.memory_size, batch_shape=[self.batch_size]))  # making sure self links are off\n",
    "        return tf.identity(lm, name=\"Link_matrix\")\n",
    "\n",
    "    @staticmethod\n",
    "    def content_based_addressing(memory, keys, strength):\n",
    "        keys = tf.nn.l2_normalize(keys, dim=2)\n",
    "        memory = tf.nn.l2_normalize(memory, dim=2)\n",
    "        similarity = tf.einsum(\"bnw,brw,br->bnr\", memory, keys, strength)\n",
    "        content_weighting = tf.nn.softmax(similarity, dim=1, name=\"Content_weighting\")\n",
    "\n",
    "        return content_weighting\n",
    "\n",
    "    @staticmethod\n",
    "    def oneplus(x):\n",
    "        return 1 + tf.nn.softplus(x)\n",
    "\n",
    "    def reshape_and_softmax(self, r_read_modes):\n",
    "        r_read_modes = tf.reshape(r_read_modes, [self.batch_size, self.num_read_heads, 3])\n",
    "        return tf.nn.softmax(r_read_modes, dim=2)\n",
    "\n",
    "      \n",
    "#### Controller #########################################\n",
    "\n",
    "class Controller:\n",
    "\n",
    "    max_outputs = 1\n",
    "    clip_value = 10\n",
    "\n",
    "    def run_session(self,\n",
    "                    task,\n",
    "                    hp,\n",
    "                    project_path,\n",
    "                    restore_path=None,\n",
    "                    optimizer=tf.train.RMSPropOptimizer(learning_rate=1e-4, momentum=0.9)):\n",
    "\n",
    "        x = tf.placeholder(tf.float32, task.x_shape, name=\"X\")\n",
    "        y = tf.placeholder(tf.float32, task.y_shape, name=\"Y\")\n",
    "\n",
    "        sequence_lengths = tf.placeholder(tf.int32, [None], name=\"Sequence_length\")\n",
    "        mask = tf.placeholder(tf.float32, task.mask, name=\"Output_mask\")\n",
    "\n",
    "        outputs, summaries = self(x, sequence_lengths)\n",
    "        assert tf.shape(outputs).shape == tf.shape(y).shape\n",
    "\n",
    "        summary_outputs = tf.nn.sigmoid(outputs)\n",
    "\n",
    "        tf.summary.image(\"0_Input\", tf.expand_dims(tf.transpose(x, [0, 2, 1]), axis=3),\n",
    "                         max_outputs=Controller.max_outputs)\n",
    "        tf.summary.image(\"0_Network_output\", tf.expand_dims(tf.transpose(summary_outputs, [0, 2, 1]), axis=3),\n",
    "                         max_outputs=Controller.max_outputs)\n",
    "        tf.summary.image(\"0_Y\", tf.expand_dims(tf.transpose(y * mask, [0, 2, 1]), axis=3),\n",
    "                         max_outputs=Controller.max_outputs)\n",
    "\n",
    "        cost = task.cost(outputs, y, mask)\n",
    "        tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        from tensorflow.python.framework import ops\n",
    "        for i, (gradient, variable) in enumerate(gradients):\n",
    "            if gradient is not None:\n",
    "                clipped_gradient = tf.clip_by_value(gradient, -Controller.clip_value, Controller.clip_value)\n",
    "                gradients[i] = clipped_gradient, variable\n",
    "                tf.summary.histogram(variable.name, variable)\n",
    "                tf.summary.histogram(variable.name + \"/gradients\",\n",
    "                                     gradient.values if isinstance(gradient, ops.IndexedSlices) else gradient)\n",
    "        optimizer = optimizer.apply_gradients(gradients)\n",
    "\n",
    "        self.notify(summaries)\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        from numpy import prod, sum\n",
    "        n_vars = sum([prod(var.shape) for var in tf.trainable_variables()])\n",
    "        print(\"This model has\", n_vars, \"parameters!\")\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            if restore_path is not None:\n",
    "                saver.restore(sess, restore_path)\n",
    "                print(\"Restored model\", restore_path, \"!!!!!!!!!!!!!\")\n",
    "            else:\n",
    "                tf.global_variables_initializer().run()\n",
    "            train_writer = tf.summary.FileWriter(project_path.train_path, sess.graph)\n",
    "            test_writer = tf.summary.FileWriter(project_path.test_path, sess.graph)\n",
    "\n",
    "            from time import time\n",
    "            t = time()\n",
    "\n",
    "            cost_value = 9999\n",
    "            print(\"Starting...\")\n",
    "            for step in range(hp.steps):\n",
    "                data_batch, seqlen, m = task.generate_data(cost=cost_value, batch_size=hp.batch_size, train=True)\n",
    "                _, cost_value = sess.run([optimizer, cost],\n",
    "                                         feed_dict={x: data_batch[0], y: data_batch[1], sequence_lengths: seqlen,\n",
    "                                                    mask: m})\n",
    "                if step % 100 == 0:\n",
    "                    summary = sess.run(merged, feed_dict={x: data_batch[0], y: data_batch[1],\n",
    "                                                          sequence_lengths: seqlen,\n",
    "                                                          mask: m})\n",
    "                    train_writer.add_summary(summary, step)\n",
    "                    test_data_batch, seqlen, m = task.generate_data(cost=cost, train=False, batch_size=hp.batch_size)\n",
    "                    summary, pred, cost_value = sess.run([merged, outputs, cost],\n",
    "                                                         feed_dict={x: test_data_batch[0], y: test_data_batch[1],\n",
    "                                                                    sequence_lengths: seqlen, mask: m})\n",
    "                    test_writer.add_summary(summary, step)\n",
    "                    task.display_output(pred, test_data_batch, m)\n",
    "\n",
    "                    print(\"Summary generated. Step\", step,\n",
    "                          \" Test cost == %.9f Time == %.2fs\" % (cost_value, time() - t))\n",
    "                    t = time()\n",
    "\n",
    "                    if step % 5000 == 0 and step > 0:\n",
    "                        task.test(sess, outputs, [x, y, sequence_lengths, mask], hp.batch_size)\n",
    "                        saver.save(sess, project_path.model_path)\n",
    "                        print(\"Model saved!\")\n",
    "\n",
    "        \n",
    "class LSTM(Controller):\n",
    "    def __init__(self, inp_vector_size, memory_size, n_layers, out_vector_size=None, initializer=tf.random_normal,\n",
    "                 initial_stddev=None):\n",
    "        self.inp_vector_size = inp_vector_size\n",
    "        self.memory_size = memory_size\n",
    "        self.out_vector_size = self.memory_size\n",
    "        self.out_layer_exists = False\n",
    "        self.n_layers = n_layers\n",
    "        if out_vector_size is not None:\n",
    "            self.out_vector_size = out_vector_size\n",
    "            self.out_layer_exists = True\n",
    "            self.weights = tf.Variable(initializer([self.memory_size, self.out_vector_size], stddev=initial_stddev),\n",
    "                                       name=\"output_weights\")\n",
    "            self.biases = tf.Variable(tf.zeros([self.out_vector_size]), name=\"output_biases\")\n",
    "\n",
    "        one_cell = tf.contrib.rnn.BasicLSTMCell\n",
    "        self.lstm_cell = tf.contrib.rnn.MultiRNNCell([one_cell(self.memory_size) for _ in range(self.n_layers)])\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        return self.lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    def __call__(self, x, sequence_lengths):\n",
    "\n",
    "        outputs, states = tf.nn.dynamic_rnn(self.lstm_cell,\n",
    "                                            x,\n",
    "                                            dtype=tf.float32,\n",
    "                                            sequence_length=sequence_lengths,\n",
    "                                            swap_memory=True)\n",
    "        if self.out_layer_exists:\n",
    "            outputs = tf.einsum(\"btm,mo->bto\", outputs, self.weights) + self.biases\n",
    "\n",
    "        return outputs, states\n",
    "\n",
    "    def step(self, x, state, step):\n",
    "        with tf.variable_scope(\"LSTM_step\"):\n",
    "            hidden, new_state = self.lstm_cell(x, state)\n",
    "        return hidden, new_state\n",
    "\n",
    "    def notify(self, summaries):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "#### DNC ################################################\n",
    "\n",
    "class DNC(Controller):\n",
    "    def __init__(self, controller, batch_size, out_vector_size, mem_hp, initializer=tf.random_normal,\n",
    "                 initial_stddev=0.1):\n",
    "        self.controller = controller\n",
    "        self.batch_size = batch_size\n",
    "        self.out_vector_size = out_vector_size\n",
    "        self.controller_output_size = self.controller.out_vector_size\n",
    "\n",
    "        self.mem_hp = mem_hp\n",
    "\n",
    "        self.memory = Memory(self.batch_size, self.controller_output_size, self.out_vector_size, self.mem_hp,\n",
    "                             initializer=initializer, initial_stddev=initial_stddev)\n",
    "\n",
    "        self.output_weights = tf.Variable(\n",
    "            initializer([self.controller.out_vector_size, self.out_vector_size], stddev=initial_stddev),\n",
    "            name=\"output_weights_controller\")\n",
    "\n",
    "    def __call__(self, x, sequence_length):\n",
    "        \"\"\"\n",
    "        Performs the DNC calculation for all time steps of the input data (x).\n",
    "        \"\"\"\n",
    "        batch_size = self.batch_size\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        with tf.variable_scope(\"DNC\"):\n",
    "            condition = lambda step, *_: step < seq_len\n",
    "\n",
    "            initial_state = [self.controller.initial_state(batch_size), self.memory.init_memory(batch_size)]\n",
    "            output_initial = tf.zeros((batch_size, self.out_vector_size))\n",
    "\n",
    "            all_outputs = tf.TensorArray(tf.float32, seq_len)\n",
    "            all_summaries = [tf.TensorArray(tf.float32, seq_len)\n",
    "                             for _ in range(len(initial_state[1]) + 13)]\n",
    "\n",
    "            step, x, output, state, all_outputs, all_summaries = tf.while_loop(condition,\n",
    "                                                                               self.while_loop_step,\n",
    "                                                                               loop_vars=[0,\n",
    "                                                                                          x,\n",
    "                                                                                          output_initial,\n",
    "                                                                                          initial_state,\n",
    "                                                                                          all_outputs,\n",
    "                                                                                          all_summaries],\n",
    "                                                                               swap_memory=True)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), [1, 0, 2])\n",
    "        all_summaries = [summary.stack() for summary in all_summaries]\n",
    "        return all_outputs, all_summaries\n",
    "\n",
    "    def while_loop_step(self, step, x, output, state, all_outputs, all_summaries):\n",
    "        \"\"\"\n",
    "        Processes one time step of DNC and keeps track of all needed variables (including ones for visualization in tb)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x_step = x[:, step, :]\n",
    "\n",
    "        read_vectors_flat = tf.reshape(state[1][6],\n",
    "                                       [batch_size, self.memory.num_read_heads * self.memory.word_size])\n",
    "        controller_input = tf.concat([x_step, read_vectors_flat], axis=1)\n",
    "        controller_state, memory_state = state\n",
    "\n",
    "        controller_output, controller_state = self.controller.step(controller_input, controller_state, step)\n",
    "        memory_output, memory_state, extra_images = self.memory.step(controller_output, memory_state)\n",
    "\n",
    "        output_vector = controller_output @ self.output_weights\n",
    "        output = output_vector + memory_output\n",
    "\n",
    "        state = [controller_state, memory_state]\n",
    "        all_outputs = all_outputs.write(step, output)\n",
    "        new_summaries = [all_summaries[i].write(step, state) for i, state in enumerate(memory_state)]\n",
    "        new_summaries.extend(\n",
    "            [all_summaries[len(memory_state) + i].write(step, summ_img) for i, summ_img in enumerate(extra_images)])\n",
    "        new_summaries.extend([all_summaries[-2].write(step, memory_output)])\n",
    "        new_summaries.extend([all_summaries[-1].write(step, controller_state)])\n",
    "        return [step + 1, x, output, state, all_outputs, new_summaries]\n",
    "\n",
    "\n",
    "    def notify(self, summaries):\n",
    "        \"\"\"\n",
    "        Processes all the tensors for display in tensorboard\n",
    "        \"\"\"\n",
    "        if self.controller is not None:\n",
    "            self.controller.notify(summaries[-1])\n",
    "\n",
    "        def summary_convert(summary, title):\n",
    "            tf.summary.image(title, tf.expand_dims(tf.transpose(summary, [1, 2, 0]), axis=3),\n",
    "                             max_outputs=Controller.max_outputs)\n",
    "\n",
    "        n = self.mem_hp.mem_size\n",
    "        r = self.mem_hp.num_read_heads\n",
    "        summaries[0] = tf.reshape(tf.transpose(summaries[0], [0, 1, 3, 2]), [-1, self.batch_size, n * r])\n",
    "        summaries[6] = tf.reshape(summaries[6],\n",
    "                                  [-1, self.batch_size, self.mem_hp.num_read_heads * self.mem_hp.word_size])\n",
    "        summaries[7] = tf.reshape(summaries[7], [-1, self.batch_size, self.mem_hp.num_read_heads * 3])\n",
    "        summaries[14] = tf.reshape(tf.transpose(summaries[14], [0, 1, 3, 2]), [-1, self.batch_size, n * r])\n",
    "        summaries[15] = tf.reshape(tf.transpose(summaries[15], [0, 1, 3, 2]), [-1, self.batch_size, n * r])\n",
    "        summaries[17] = tf.reshape(summaries[17],\n",
    "                                   [-1, self.batch_size, self.mem_hp.num_read_heads * self.mem_hp.word_size])\n",
    "\n",
    "        summary_names = [\"Read_weightings\",\n",
    "                         \"Write_weighting\",\n",
    "                         \"Usage_vector\",\n",
    "                         \"Precedence_weighting\",\n",
    "                         \"Memory_matrix\",\n",
    "                         \"Link_matrix\",\n",
    "                         \"R_read_vectors\",\n",
    "                         \"R_read_modes\",\n",
    "                         \"Write_gate\",\n",
    "                         \"Allocation_gate\",\n",
    "                         \"Write_strength\",\n",
    "                         \"R_read_strengths\",\n",
    "                         \"Erase_vector\",\n",
    "                         \"Write_vector\",\n",
    "                         \"Forward_weighting\",\n",
    "                         \"Backward_weighting\",\n",
    "                         \"R_free_gates\",\n",
    "                         \"R_read_keys\",\n",
    "                         \"Memory_output\"\n",
    "                         ]\n",
    "        for i, summ_name in enumerate(summary_names):\n",
    "            if i not in [4, 5, 19]:\n",
    "                summary_convert(summaries[i], summ_name)\n",
    "        \n",
    "\n",
    "#### main ###############################################        \n",
    "        \n",
    "tf.reset_default_graph()\n",
    "\n",
    "weight_initializer = init_wrapper(tf.random_normal)\n",
    "n_blocks = 6\n",
    "vector_size = n_blocks + 1\n",
    "min_seq = 5\n",
    "train_max_seq = 6\n",
    "n_copies = 1\n",
    "out_vector_size = vector_size\n",
    "project_path = ProjectPath(\"log\")\n",
    "\n",
    "task = bAbITask()\n",
    "\n",
    "print(\"Loaded task\")\n",
    "\n",
    "class Param:\n",
    "    batch_size = 1\n",
    "    steps = 20000\n",
    "    lstm_memory_size = 256\n",
    "    n_layers = 1\n",
    "    stddev = 0.1\n",
    "\n",
    "    class Mem:\n",
    "        word_size = 32\n",
    "        mem_size = 256\n",
    "        num_read_heads = 4\n",
    "\n",
    "\n",
    "controller = LSTM(task.vector_size, Param.lstm_memory_size, Param.n_layers, initializer=weight_initializer, initial_stddev=Param.stddev)\n",
    "dnc = DNC(controller, Param.batch_size, task.vector_size, Param.Mem, initializer=weight_initializer, initial_stddev=Param.stddev)\n",
    "\n",
    "dnc.run_session(task, Param, project_path)  # , restore_path=restore_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4ACsOf3vrwl"
   },
   "source": [
    "## Recurrent Entity Network\n",
    "\n",
    "Recurrent entity networks (EntNets) incorporate a fixed bank of dynamic memory cells that allow simultaneous location and content-based updates. Because of this ability, they perform very well and set the state-of-the-art in reasoning tasks such as bAbI. Unlike the DNC which relies on a sophisticated central controller, EntNet is essentially a set of separate, parallel recurrent memories with independent gates for each memory.\n",
    "\n",
    "The EntNet architecture consists of an input encoder, a dynamic memory, and an output layer. It operates with the following steps:\n",
    "\n",
    "1. The input story sentences and query are mapped to embedding representations and passed to the dynamic memory layer and output layer, respectively\n",
    "2. Key vectors with the embeddings of entities are generated\n",
    "3. The hidden states (memories) of the set of gated GRU blocks within the dynamic memory are updated over the input encoder vectors and key vectors\n",
    "4. The output layer applies a softmax over the query q and hidden states of the memory cells to generate a probability distribution over the potential answers\n",
    "5. The entire network is trained to minimize the error between the output layer candidate and answer\n",
    "\n",
    "We train an EntNet model (https://github.com/jimfleming/recurrent-entity-networks) using the extended bAbI training set with 100-dim embeddings, 20 blocks, batch size of 32, and the ADAM optimizer with gradient clipping for 200 epochs. Note that with proper hyperparameter tuning, the performance of EntNet and the previous architectures can be improved on the bAbI tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "mPq9ehvnvrwm",
    "outputId": "21409690-ac39-470b-be0e-8e9309655a5b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "SPLIT_RE = re.compile(r'(\\W+)?')\n",
    "\n",
    "PAD_TOKEN = '_PAD'\n",
    "PAD_ID = 0\n",
    "source_path = \"babi_tasks_1-20_v1-2.tar.gz\"\n",
    "output_dir = \"tasks_1-20_v1-2\"\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"Tokenize a string by splitting on non-word characters and stripping whitespace.\"\n",
    "    return [token.strip().lower() for token in re.split(SPLIT_RE, sentence) if token.strip()]\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    \"\"\"\n",
    "    Parse the bAbI task format described here: https://research.facebook.com/research/babi/\n",
    "    If only_supporting is True, only the sentences that support the answer are kept.\n",
    "    \"\"\"\n",
    "    stories = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            query, answer, supporting = line.split('\\t')\n",
    "            query = tokenize(query)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            stories.append((substory, query, answer))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sentence = tokenize(line)\n",
    "            story.append(sentence)\n",
    "    return stories\n",
    "\n",
    "def save_dataset(stories, path):\n",
    "    \"\"\"\n",
    "    Save the stories into TFRecords.\n",
    "    NOTE: Since each sentence is a consistent length from padding, we use\n",
    "    `tf.train.Example`, rather than a `tf.train.SequenceExample`, which is\n",
    "    _slightly_ faster.\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(path)\n",
    "    for story, query, answer in stories:\n",
    "        story_flat = [token_id for sentence in story for token_id in sentence]\n",
    "\n",
    "        story_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=story_flat))\n",
    "        query_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=query))\n",
    "        answer_feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[answer]))\n",
    "\n",
    "        features = tf.train.Features(feature={\n",
    "            'story': story_feature,\n",
    "            'query': query_feature,\n",
    "            'answer': answer_feature,\n",
    "        })\n",
    "\n",
    "        example = tf.train.Example(features=features)\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "def tokenize_stories(stories, token_to_id):\n",
    "    \"Convert all tokens into their unique ids.\"\n",
    "    story_ids = []\n",
    "    for story, query, answer in stories:\n",
    "        story = [[token_to_id[token] for token in sentence] for sentence in story]\n",
    "        query = [token_to_id[token] for token in query]\n",
    "        answer = token_to_id[answer]\n",
    "        story_ids.append((story, query, answer))\n",
    "    return story_ids\n",
    "\n",
    "def get_tokenizer(stories):\n",
    "    \"Recover unique tokens as a vocab and map the tokens to ids.\"\n",
    "    tokens_all = []\n",
    "    for story, query, answer in stories:\n",
    "        tokens_all.extend([token for sentence in story for token in sentence] + query + [answer])\n",
    "    vocab = [PAD_TOKEN] + sorted(set(tokens_all))\n",
    "    token_to_id = {token: i for i, token in enumerate(vocab)}\n",
    "    return vocab, token_to_id\n",
    "\n",
    "def pad_stories(stories, max_sentence_length, max_story_length, max_query_length):\n",
    "    \"Pad sentences, stories, and queries to a consistence length.\"\n",
    "    for story, query, _ in stories:\n",
    "        for sentence in story:\n",
    "            for _ in range(max_sentence_length - len(sentence)):\n",
    "                sentence.append(PAD_ID)\n",
    "            assert len(sentence) == max_sentence_length\n",
    "\n",
    "        for _ in range(max_story_length - len(story)):\n",
    "            story.append([PAD_ID for _ in range(max_sentence_length)])\n",
    "\n",
    "        for _ in range(max_query_length - len(query)):\n",
    "            query.append(PAD_ID)\n",
    "\n",
    "        assert len(story) == max_story_length\n",
    "        assert len(query) == max_query_length\n",
    "\n",
    "    return stories\n",
    "\n",
    "def truncate_stories(stories, max_length):\n",
    "    \"Truncate a story to the specified maximum length.\"\n",
    "    stories_truncated = []\n",
    "    for story, query, answer in stories:\n",
    "        story_truncated = story[-max_length:]\n",
    "        stories_truncated.append((story_truncated, query, answer))\n",
    "    return stories_truncated\n",
    "\n",
    "## ETL\n",
    "\n",
    "\n",
    "task_names = [\n",
    "'qa1_single-supporting-fact',\n",
    "'qa2_two-supporting-facts',\n",
    "'qa3_three-supporting-facts',\n",
    "]\n",
    "\n",
    "task_titles = [\n",
    "'Task 1: Single Supporting Fact',\n",
    "'Task 2: Two Supporting Facts',\n",
    "'Task 3: Three Supporting Facts',\n",
    "]\n",
    "\n",
    "task_ids = [\n",
    "'qa1',\n",
    "'qa2',\n",
    "'qa3',\n",
    "]\n",
    "\n",
    "for task_id, task_name, task_title in tqdm(zip(task_ids, task_names, task_titles), \\\n",
    "    desc='Processing datasets into records...'):\n",
    "    stories_path_train = os.path.join('tasks_1-20_v1-2/en-10k/', task_name + '_train.txt')\n",
    "    stories_path_test = os.path.join('tasks_1-20_v1-2/en-10k/', task_name + '_test.txt')\n",
    "    dataset_path_train = os.path.join(output_dir, task_id + '_10k_train.tfrecords')\n",
    "    dataset_path_test = os.path.join(output_dir, task_id + '_10k_test.tfrecords')\n",
    "    metadata_path = os.path.join(output_dir, task_id + '_10k.json')\n",
    "    task_size = 10000\n",
    "\n",
    "    if task_id == 'qa3':\n",
    "        truncated_story_length = 130\n",
    "    else:\n",
    "        truncated_story_length = 70\n",
    "\n",
    "    tar = tarfile.open(source_path)\n",
    "\n",
    "    f_train = tar.extractfile(stories_path_train)\n",
    "    f_test = tar.extractfile(stories_path_test)\n",
    "\n",
    "    stories_train = parse_stories(f_train.readlines())\n",
    "    stories_test = parse_stories(f_test.readlines())\n",
    "\n",
    "    stories_train = truncate_stories(stories_train, truncated_story_length)\n",
    "    stories_test = truncate_stories(stories_test, truncated_story_length)\n",
    "\n",
    "    vocab, token_to_id = get_tokenizer(stories_train + stories_test)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    stories_token_train = tokenize_stories(stories_train, token_to_id)\n",
    "    stories_token_test = tokenize_stories(stories_test, token_to_id)\n",
    "    stories_token_all = stories_token_train + stories_token_test\n",
    "\n",
    "    story_lengths = [len(sentence) for story, _, _ in stories_token_all for sentence in story]\n",
    "    max_sentence_length = max(story_lengths)\n",
    "    max_story_length = max([len(story) for story, _, _ in stories_token_all])\n",
    "    max_query_length = max([len(query) for _, query, _ in stories_token_all])\n",
    "\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        metadata = {\n",
    "            'task_id': task_id,\n",
    "            'task_name': task_name,\n",
    "            'task_title': task_title,\n",
    "            'task_size': task_size,\n",
    "            'max_query_length': max_query_length,\n",
    "            'max_story_length': max_story_length,\n",
    "            'max_sentence_length': max_sentence_length,\n",
    "            'vocab': vocab,\n",
    "            'vocab_size': vocab_size,\n",
    "            'filenames': {\n",
    "                'train': os.path.basename(dataset_path_train),\n",
    "                'test': os.path.basename(dataset_path_test),\n",
    "            }\n",
    "        }\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    stories_pad_train = pad_stories(stories_token_train, \\\n",
    "        max_sentence_length, max_story_length, max_query_length)\n",
    "    stories_pad_test = pad_stories(stories_token_test, \\\n",
    "        max_sentence_length, max_story_length, max_query_length)\n",
    "\n",
    "    save_dataset(stories_pad_train, dataset_path_train)\n",
    "    save_dataset(stories_pad_test, dataset_path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WU8g_ZPwLK67"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.training import basic_session_run_hooks\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_BLOCKS = 20\n",
    "EMBEDDING_SIZE = 100\n",
    "CLIP_GRADIENTS = 40.0\n",
    "OPTIMIZER_SUMMARIES = [\n",
    "    \"learning_rate\",\n",
    "    \"loss\",\n",
    "    \"gradients\",\n",
    "    \"gradient_norm\",\n",
    "]\n",
    "\n",
    "\n",
    "class EarlyStoppingHook(tf.train.SessionRunHook):\n",
    "\n",
    "    def __init__(self, input_fn, estimator, metrics,\n",
    "                 metric_name='loss', every_steps=100,\n",
    "                 max_patience=100, minimize=True):\n",
    "        self._input_fn = input_fn\n",
    "        self._estimator = estimator\n",
    "        self._metrics = metrics\n",
    "\n",
    "        self._metric_name = metric_name\n",
    "        self._every_steps = every_steps\n",
    "        self._max_patience = max_patience\n",
    "        self._minimize = minimize\n",
    "\n",
    "        self._timer = basic_session_run_hooks.SecondOrStepTimer(\n",
    "            every_steps=every_steps,\n",
    "            every_secs=None)\n",
    "\n",
    "        self._global_step = None\n",
    "        self._best_value = None\n",
    "        self._best_step = None\n",
    "\n",
    "    def begin(self):\n",
    "        self._global_step = tf.train.get_global_step()\n",
    "        if self._global_step is None:\n",
    "            raise RuntimeError('Global step should be created to use EarlyStoppingHook.')\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        return tf.train.SessionRunArgs(self._global_step)\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        global_step = run_values.results\n",
    "\n",
    "        if not self._timer.should_trigger_for_step(global_step):\n",
    "            return\n",
    "\n",
    "        self._timer.update_last_triggered_step(global_step)\n",
    "\n",
    "        results = self._estimator.evaluate(\n",
    "            input_fn=self._input_fn,\n",
    "            metrics=self._metrics)\n",
    "\n",
    "        if self._metric_name not in results:\n",
    "            raise ValueError('Metric {} missing from outputs {}.' \\\n",
    "                .format(self._metric_name, set(results.keys())))\n",
    "\n",
    "        current_value = results[self._metric_name]\n",
    "\n",
    "        if (self._best_value is None) or \\\n",
    "           (self._minimize and current_value < self._best_value) or \\\n",
    "           (not self._minimize and current_value > self._best_value):\n",
    "            self._best_value = current_value\n",
    "            self._best_step = global_step\n",
    "\n",
    "        should_stop = (global_step - self._best_step >= self._max_patience)\n",
    "        if should_stop:\n",
    "            print('Stopping... Best step: {} with {} = {}.' \\\n",
    "                .format(self._best_step, self._metric_name, self._best_value))\n",
    "            run_context.request_stop()\n",
    "            \n",
    "            \n",
    "def generate_input_fn(filename, metadata, batch_size, num_epochs=None, shuffle=False):\n",
    "    \"Return _input_fn for use with Experiment.\"\n",
    "    def _input_fn():\n",
    "        max_story_length = metadata['max_story_length']\n",
    "        max_sentence_length = metadata['max_sentence_length']\n",
    "        max_query_length = metadata['max_query_length']\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            story_feature = tf.FixedLenFeature(\n",
    "                shape=[max_story_length, max_sentence_length],\n",
    "                dtype=tf.int64)\n",
    "            query_feature = tf.FixedLenFeature(\n",
    "                shape=[1, max_query_length],\n",
    "                dtype=tf.int64)\n",
    "            answer_feature = tf.FixedLenFeature(\n",
    "                shape=[],\n",
    "                dtype=tf.int64)\n",
    "\n",
    "            features = {\n",
    "                'story': story_feature,\n",
    "                'query': query_feature,\n",
    "                'answer': answer_feature,\n",
    "            }\n",
    "\n",
    "            record_features = tf.contrib.learn.read_batch_record_features(\n",
    "                file_pattern=filename,\n",
    "                features=features,\n",
    "                batch_size=batch_size,\n",
    "                randomize_input=shuffle,\n",
    "                num_epochs=num_epochs)\n",
    "\n",
    "            story = record_features['story']\n",
    "            query = record_features['query']\n",
    "            answer = record_features['answer']\n",
    "\n",
    "            features = {\n",
    "                'story': story,\n",
    "                'query': query,\n",
    "            }\n",
    "\n",
    "            return features, answer\n",
    "\n",
    "    return _input_fn            \n",
    "            \n",
    "    \n",
    "def generate_serving_input_fn(metadata):\n",
    "    \"Returns _serving_input_fn for use with an export strategy.\"\n",
    "    max_story_length = metadata['max_story_length']\n",
    "    max_sentence_length = metadata['max_sentence_length']\n",
    "    max_query_length = metadata['max_query_length']\n",
    "\n",
    "    def _serving_input_fn():\n",
    "        story_placeholder = tf.placeholder(\n",
    "            shape=[max_story_length, max_sentence_length],\n",
    "            dtype=tf.int64,\n",
    "            name='story')\n",
    "        query_placeholder = tf.placeholder(\n",
    "            shape=[1, max_query_length],\n",
    "            dtype=tf.int64,\n",
    "            name='query')\n",
    "\n",
    "        feature_placeholders = {\n",
    "            'story': story_placeholder,\n",
    "            'query': query_placeholder\n",
    "        }\n",
    "\n",
    "        features = {\n",
    "            key: tf.expand_dims(tensor, axis=0)\n",
    "            for key, tensor in feature_placeholders.items()\n",
    "        }\n",
    "\n",
    "        input_fn_ops = tf.contrib.learn.utils.input_fn_utils.InputFnOps(\n",
    "            features=features,\n",
    "            labels=None,\n",
    "            default_inputs=feature_placeholders)\n",
    "\n",
    "        return input_fn_ops\n",
    "\n",
    "    return _serving_input_fn    \n",
    "\n",
    "  \n",
    "def get_input_encoding(inputs, initializer=None, scope=None):\n",
    "    \"\"\"\n",
    "    Implementation of the learned multiplicative mask from Section 2.1, Equation 1.\n",
    "    This module is also described in [End-To-End Memory Networks](https://arxiv.org/abs/1502.01852)\n",
    "    as Position Encoding (PE). The mask allows the ordering of words in a sentence to affect the\n",
    "    encoding.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'Encoding', initializer=initializer):\n",
    "        _, _, max_sentence_length, embedding_size = inputs.get_shape().as_list()\n",
    "        positional_mask = tf.get_variable(\n",
    "            name='positional_mask',\n",
    "            shape=[max_sentence_length, embedding_size])\n",
    "        encoded_input = tf.reduce_sum(inputs * positional_mask, axis=2)\n",
    "        return encoded_input\n",
    "\n",
    "def get_output_module(\n",
    "        last_state,\n",
    "        encoded_query,\n",
    "        num_blocks,\n",
    "        vocab_size,\n",
    "        activation=tf.nn.relu,\n",
    "        initializer=None,\n",
    "        scope=None):\n",
    "    \"\"\"\n",
    "    Implementation of Section 2.3, Equation 6. This module is also described in more detail here:\n",
    "    [End-To-End Memory Networks](https://arxiv.org/abs/1502.01852).\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'Output', initializer=initializer):\n",
    "        last_state = tf.stack(tf.split(last_state, num_blocks, axis=1), axis=1)\n",
    "        _, _, embedding_size = last_state.get_shape().as_list()\n",
    "\n",
    "        # Use the encoded_query to attend over memories\n",
    "        # (hidden states of dynamic last_state cell blocks)\n",
    "        attention = tf.reduce_sum(last_state * encoded_query, axis=2)\n",
    "\n",
    "        # Subtract max for numerical stability (softmax is shift invariant)\n",
    "        attention_max = tf.reduce_max(attention, axis=-1, keep_dims=True)\n",
    "        attention = tf.nn.softmax(attention - attention_max)\n",
    "        attention = tf.expand_dims(attention, axis=2)\n",
    "\n",
    "        # Weight memories by attention vectors\n",
    "        u = tf.reduce_sum(last_state * attention, axis=1)\n",
    "\n",
    "        # R acts as the decoder matrix to convert from internal state to the output vocabulary size\n",
    "        R = tf.get_variable('R', [embedding_size, vocab_size])\n",
    "        H = tf.get_variable('H', [embedding_size, embedding_size])\n",
    "\n",
    "        q = tf.squeeze(encoded_query, axis=1)\n",
    "        y = tf.matmul(activation(q + tf.matmul(u, H)), R)\n",
    "        return y\n",
    "    outputs = None\n",
    "    return outputs\n",
    "\n",
    "def get_outputs(inputs, params):\n",
    "    \"Return the outputs from the model which will be used in the loss function.\"\n",
    "    embedding_size = params['embedding_size']\n",
    "    num_blocks = params['num_blocks']\n",
    "    vocab_size = params['vocab_size']\n",
    "\n",
    "    story = inputs['story']\n",
    "    query = inputs['query']\n",
    "\n",
    "    batch_size = tf.shape(story)[0]\n",
    "\n",
    "    normal_initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "    ones_initializer = tf.constant_initializer(1.0)\n",
    "\n",
    "    # Extend the vocab to include keys for the dynamic memory cell,\n",
    "    # allowing the initialization of the memory to be learned.\n",
    "    vocab_size = vocab_size + num_blocks\n",
    "\n",
    "    with tf.variable_scope('EntityNetwork', initializer=normal_initializer):\n",
    "        # PReLU activations have their alpha parameters initialized to 1\n",
    "        # so they may be identity before training.\n",
    "        alpha = tf.get_variable(\n",
    "            name='alpha',\n",
    "            shape=embedding_size,\n",
    "            initializer=ones_initializer)\n",
    "        activation = partial(prelu, alpha=alpha)\n",
    "\n",
    "        # Embeddings\n",
    "        embedding_params = tf.get_variable(\n",
    "            name='embedding_params',\n",
    "            shape=[vocab_size, embedding_size])\n",
    "\n",
    "        # The embedding mask forces the special \"pad\" embedding to zeros.\n",
    "        embedding_mask = tf.constant(\n",
    "            value=[0 if i == 0 else 1 for i in range(vocab_size)],\n",
    "            shape=[vocab_size, 1],\n",
    "            dtype=tf.float32)\n",
    "        embedding_params_masked = embedding_params * embedding_mask\n",
    "\n",
    "        story_embedding = tf.nn.embedding_lookup(embedding_params_masked, story)\n",
    "        query_embedding = tf.nn.embedding_lookup(embedding_params_masked, query)\n",
    "\n",
    "        # Input Module\n",
    "        encoded_story = get_input_encoding(\n",
    "            inputs=story_embedding,\n",
    "            initializer=ones_initializer,\n",
    "            scope='StoryEncoding')\n",
    "        encoded_query = get_input_encoding(\n",
    "            inputs=query_embedding,\n",
    "            initializer=ones_initializer,\n",
    "            scope='QueryEncoding')\n",
    "\n",
    "        # Memory Module\n",
    "        # We define the keys outside of the cell so they may be used for memory initialization.\n",
    "        # Keys are initialized to a range outside of the main vocab.\n",
    "        keys = [key for key in range(vocab_size - num_blocks, vocab_size)]\n",
    "        keys = tf.nn.embedding_lookup(embedding_params_masked, keys)\n",
    "        keys = tf.split(keys, num_blocks, axis=0)\n",
    "        keys = [tf.squeeze(key, axis=0) for key in keys]\n",
    "\n",
    "        cell = DynamicMemoryCell(\n",
    "            num_blocks=num_blocks,\n",
    "            num_units_per_block=embedding_size,\n",
    "            keys=keys,\n",
    "            initializer=normal_initializer,\n",
    "            recurrent_initializer=normal_initializer,\n",
    "            activation=activation)\n",
    "\n",
    "        # Recurrence\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        sequence_length = get_sequence_length(encoded_story)\n",
    "        _, last_state = tf.nn.dynamic_rnn(\n",
    "            cell=cell,\n",
    "            inputs=encoded_story,\n",
    "            sequence_length=sequence_length,\n",
    "            initial_state=initial_state)\n",
    "\n",
    "        # Output Module\n",
    "        outputs = get_output_module(\n",
    "            last_state=last_state,\n",
    "            encoded_query=encoded_query,\n",
    "            num_blocks=num_blocks,\n",
    "            vocab_size=vocab_size,\n",
    "            initializer=normal_initializer,\n",
    "            activation=activation)\n",
    "\n",
    "        parameters = count_parameters()\n",
    "        print('Parameters: {}'.format(parameters))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def get_predictions(outputs):\n",
    "    \"Return the actual predictions for use with evaluation metrics or TF Serving.\"\n",
    "    predictions = tf.argmax(outputs, axis=-1)\n",
    "    return predictions\n",
    "\n",
    "def get_loss(outputs, labels, mode):\n",
    "    \"Return the loss function which will be used with an optimizer.\"\n",
    "\n",
    "    loss = None\n",
    "    if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "        return loss\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        logits=outputs,\n",
    "        labels=labels)\n",
    "    return loss\n",
    "\n",
    "def get_train_op(loss, params, mode):\n",
    "    \"Return the trainining operation which will be used to train the model.\"\n",
    "\n",
    "    train_op = None\n",
    "    if mode != tf.contrib.learn.ModeKeys.TRAIN:\n",
    "        return train_op\n",
    "\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    learning_rate = cyclic_learning_rate(\n",
    "        learning_rate_min=params['learning_rate_min'],\n",
    "        learning_rate_max=params['learning_rate_max'],\n",
    "        step_size=params['learning_rate_step_size'],\n",
    "        mode='triangular',\n",
    "        global_step=global_step)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=global_step,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer='Adam',\n",
    "        clip_gradients=params['clip_gradients'],\n",
    "        gradient_noise_scale=params['gradient_noise_scale'],\n",
    "        summaries=OPTIMIZER_SUMMARIES)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \"Return ModelFnOps for use with Estimator.\"\n",
    "\n",
    "    outputs = get_outputs(features, params)\n",
    "    predictions = get_predictions(outputs)\n",
    "    loss = get_loss(outputs, labels, mode)\n",
    "    train_op = get_train_op(loss, params, mode)\n",
    "\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        mode=mode)  \n",
    "  \n",
    "def count_parameters():\n",
    "    \"Count the number of parameters listed under TRAINABLE_VARIABLES.\"\n",
    "    num_parameters = sum([np.prod(tvar.get_shape().as_list())\n",
    "                          for tvar in tf.trainable_variables()])\n",
    "    return num_parameters\n",
    "\n",
    "def get_sequence_length(sequence, scope=None):\n",
    "    \"Determine the length of a sequence that has been padded with zeros.\"\n",
    "    with tf.variable_scope(scope, 'SequenceLength'):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=[-1]))\n",
    "        length = tf.cast(tf.reduce_sum(used, reduction_indices=[-1]), tf.int32)\n",
    "        return length\n",
    "\n",
    "def cyclic_learning_rate(\n",
    "        learning_rate_min,\n",
    "        learning_rate_max,\n",
    "        step_size,\n",
    "        global_step,\n",
    "        mode='triangular',\n",
    "        scope=None):\n",
    "    with tf.variable_scope(scope, 'CyclicLearningRate'):\n",
    "        cycle = tf.floor(1 + tf.to_float(global_step) / (2 * step_size))\n",
    "\n",
    "        if mode == 'triangular':\n",
    "            scale = 1\n",
    "        elif mode == 'triangular2':\n",
    "            scale = 2**(cycle - 1)\n",
    "        else:\n",
    "            raise ValueError('Unrecognized mode: {}'.format(mode))\n",
    "\n",
    "        x = tf.abs(tf.to_float(global_step) / step_size - 2 * cycle + 1)\n",
    "        lr = learning_rate_min + (learning_rate_max - learning_rate_min) * \\\n",
    "            tf.maximum(0.0, 1 - x) / scale\n",
    "\n",
    "        return lr\n",
    "\n",
    "def prelu(features, alpha, scope=None):\n",
    "    \"\"\"\n",
    "    Implementation of [Parametric ReLU](https://arxiv.org/abs/1502.01852) borrowed from Keras.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'PReLU'):\n",
    "        pos = tf.nn.relu(features)\n",
    "        neg = alpha * (features - tf.abs(features)) * 0.5\n",
    "        return pos + neg\n",
    "  \n",
    "\n",
    "#### DM Cell ################################################\n",
    "\n",
    "class DynamicMemoryCell(tf.contrib.rnn.RNNCell):\n",
    "    \"\"\"\n",
    "    Implementation of a dynamic memory cell as a gated recurrent network.\n",
    "    The cell's hidden state is divided into blocks and each block's weights are tied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_blocks,\n",
    "                 num_units_per_block,\n",
    "                 keys,\n",
    "                 initializer=None,\n",
    "                 recurrent_initializer=None,\n",
    "                 activation=tf.nn.relu):\n",
    "        self._num_blocks = num_blocks # M\n",
    "        self._num_units_per_block = num_units_per_block # d\n",
    "        self._keys = keys\n",
    "        self._activation = activation # \\phi\n",
    "        self._initializer = initializer\n",
    "        self._recurrent_initializer = recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        \"Return the total state size of the cell, across all blocks.\"\n",
    "        return self._num_blocks * self._num_units_per_block\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        \"Return the total output size of the cell, across all blocks.\"\n",
    "        return self._num_blocks * self._num_units_per_block\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        \"Initialize the memory to the key values.\"\n",
    "        zero_state = tf.concat([tf.expand_dims(key, axis=0) for key in self._keys], axis=1)\n",
    "        zero_state_batch = tf.tile(zero_state, [batch_size, 1])\n",
    "        return zero_state_batch\n",
    "\n",
    "    def get_gate(self, state_j, key_j, inputs):\n",
    "        \"\"\"\n",
    "        Implements the gate (scalar for each block). Equation 2:\n",
    "        g_j <- \\sigma(s_t^T h_j + s_t^T w_j)\n",
    "        \"\"\"\n",
    "        a = tf.reduce_sum(inputs * state_j, axis=1)\n",
    "        b = tf.reduce_sum(inputs * key_j, axis=1)\n",
    "        return tf.sigmoid(a + b)\n",
    "\n",
    "    def get_candidate(self, state_j, key_j, inputs, U, V, W, U_bias):\n",
    "        \"\"\"\n",
    "        Represents the new memory candidate that will be weighted by the\n",
    "        gate value and combined with the existing memory. Equation 3:\n",
    "        h_j^~ <- \\phi(U h_j + V w_j + W s_t)\n",
    "        \"\"\"\n",
    "        key_V = tf.matmul(key_j, V)\n",
    "        state_U = tf.matmul(state_j, U) + U_bias\n",
    "        inputs_W = tf.matmul(inputs, W)\n",
    "        return self._activation(state_U + inputs_W + key_V)\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variable_scope(scope or type(self).__name__, initializer=self._initializer):\n",
    "            U = tf.get_variable('U', [self._num_units_per_block, self._num_units_per_block],\n",
    "                                initializer=self._recurrent_initializer)\n",
    "            V = tf.get_variable('V', [self._num_units_per_block, self._num_units_per_block],\n",
    "                                initializer=self._recurrent_initializer)\n",
    "            W = tf.get_variable('W', [self._num_units_per_block, self._num_units_per_block],\n",
    "                                initializer=self._recurrent_initializer)\n",
    "\n",
    "            U_bias = tf.get_variable('U_bias', [self._num_units_per_block])\n",
    "\n",
    "            # Split the hidden state into blocks (each U, V, W are shared across blocks).\n",
    "            state = tf.split(state, self._num_blocks, axis=1)\n",
    "\n",
    "            next_states = []\n",
    "            for j, state_j in enumerate(state): # Hidden State (j)\n",
    "                key_j = tf.expand_dims(self._keys[j], axis=0)\n",
    "                gate_j = self.get_gate(state_j, key_j, inputs)\n",
    "                candidate_j = self.get_candidate(state_j, key_j, inputs, U, V, W, U_bias)\n",
    "\n",
    "                # Equation 4: h_j <- h_j + g_j * h_j^~\n",
    "                # Perform an update of the hidden state (memory).\n",
    "                state_j_next = state_j + tf.expand_dims(gate_j, -1) * candidate_j\n",
    "\n",
    "                # Equation 5: h_j <- h_j / \\norm{h_j}\n",
    "                # Forget previous memories by normalization.\n",
    "                state_j_next_norm = tf.norm(\n",
    "                    tensor=state_j_next,\n",
    "                    ord='euclidean',\n",
    "                    axis=-1,\n",
    "                    keep_dims=True)\n",
    "                state_j_next_norm = tf.where(\n",
    "                    tf.greater(state_j_next_norm, 0.0),\n",
    "                    state_j_next_norm,\n",
    "                    tf.ones_like(state_j_next_norm))\n",
    "                state_j_next = state_j_next / state_j_next_norm\n",
    "\n",
    "                next_states.append(state_j_next)\n",
    "            state_next = tf.concat(next_states, axis=1)\n",
    "        return state_next, state_next\n",
    "\n",
    "\n",
    "def generate_experiment_fn(data_dir, dataset_id, num_epochs,\n",
    "                           learning_rate_min, learning_rate_max,\n",
    "                           learning_rate_step_size, gradient_noise_scale):\n",
    "    \"Return _experiment_fn for use with learn_runner.\"\n",
    "    def _experiment_fn(output_dir):\n",
    "        metadata_path = os.path.join(data_dir, '{}_10k.json'.format(dataset_id))\n",
    "        with tf.gfile.Open(metadata_path) as metadata_file:\n",
    "            metadata = json.load(metadata_file)\n",
    "\n",
    "        train_filename = os.path.join(data_dir, '{}_10k_{}.tfrecords'.format(dataset_id, 'train'))\n",
    "        eval_filename = os.path.join(data_dir, '{}_10k_{}.tfrecords'.format(dataset_id, 'test'))\n",
    "\n",
    "        train_input_fn = generate_input_fn(\n",
    "            filename=train_filename,\n",
    "            metadata=metadata,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=True)\n",
    "\n",
    "        eval_input_fn = generate_input_fn(\n",
    "            filename=eval_filename,\n",
    "            metadata=metadata,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "\n",
    "        vocab_size = metadata['vocab_size']\n",
    "        task_size = metadata['task_size']\n",
    "        train_steps_per_epoch = task_size // BATCH_SIZE\n",
    "\n",
    "        run_config = tf.contrib.learn.RunConfig(\n",
    "            save_summary_steps=train_steps_per_epoch,\n",
    "            save_checkpoints_steps=5 * train_steps_per_epoch,\n",
    "            save_checkpoints_secs=None)\n",
    "\n",
    "        params = {\n",
    "            'vocab_size': vocab_size,\n",
    "            'embedding_size': EMBEDDING_SIZE,\n",
    "            'num_blocks': NUM_BLOCKS,\n",
    "            'learning_rate_min': learning_rate_min,\n",
    "            'learning_rate_max': learning_rate_max,\n",
    "            'learning_rate_step_size': learning_rate_step_size * train_steps_per_epoch,\n",
    "            'clip_gradients': CLIP_GRADIENTS,\n",
    "            'gradient_noise_scale': gradient_noise_scale,\n",
    "        }\n",
    "\n",
    "        estimator = tf.contrib.learn.Estimator(\n",
    "            model_dir=output_dir,\n",
    "            model_fn=model_fn,\n",
    "            config=run_config,\n",
    "            params=params)\n",
    "\n",
    "        eval_metrics = {\n",
    "            'accuracy': tf.contrib.learn.MetricSpec(\n",
    "                metric_fn=tf.contrib.metrics.streaming_accuracy)\n",
    "        }\n",
    "\n",
    "        train_monitors = [\n",
    "            EarlyStoppingHook(\n",
    "                input_fn=eval_input_fn,\n",
    "                estimator=estimator,\n",
    "                metrics=eval_metrics,\n",
    "                metric_name='accuracy',\n",
    "                every_steps=5 * train_steps_per_epoch,\n",
    "                max_patience=50 * train_steps_per_epoch,\n",
    "                minimize=False)\n",
    "        ]\n",
    "\n",
    "        serving_input_fn = generate_serving_input_fn(metadata)\n",
    "        export_strategy = tf.contrib.learn.utils.make_export_strategy(\n",
    "            serving_input_fn)\n",
    "\n",
    "        experiment = tf.contrib.learn.Experiment(\n",
    "            estimator=estimator,\n",
    "            train_input_fn=train_input_fn,\n",
    "            eval_input_fn=eval_input_fn,\n",
    "            eval_metrics=eval_metrics,\n",
    "            train_monitors=train_monitors,\n",
    "            train_steps=None,\n",
    "            eval_steps=None,\n",
    "            export_strategies=[export_strategy],\n",
    "            min_eval_frequency=100)\n",
    "        return experiment\n",
    "\n",
    "    return _experiment_fn\n",
    "\n",
    "\n",
    "#### Main ##########################################\n",
    "\n",
    "data_dir = \"tasks_1-20_v1-2\"\n",
    "dataset_id = \"qa1\"      # Can set to 'qa2' or 'qa3'\n",
    "job_dir = \"jobs\"\n",
    "num_epochs = 200\n",
    "lr_min = .0002\n",
    "lr_max = .01\n",
    "lr_step_size = 10\n",
    "grad_noise = 0.005\n",
    "\n",
    "experiment_fn = generate_experiment_fn(\n",
    "        data_dir=data_dir,\n",
    "        dataset_id=dataset_id,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate_min=lr_min,\n",
    "        learning_rate_max=lr_max,\n",
    "        learning_rate_step_size=lr_step_size,\n",
    "        gradient_noise_scale=grad_noise)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "learn_runner.run(experiment_fn, job_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter 9 - Question Answering.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
